Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 5728 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-59128fd4-47bf-43f0-a96c-f814b5a48a78-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 59128fd4-47bf-43f0-a96c-f814b5a48a78
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636946208423
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-b7624fd8-4af4-401b-94a3-05cf0b554882-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b7624fd8-4af4-401b-94a3-05cf0b554882
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636946208437
Started App in 1.366 seconds (JVM running for 3.18)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 6320 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-83b14a17-7450-4d80-bc0a-bb81a35b37d5-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 83b14a17-7450-4d80-bc0a-bb81a35b37d5
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636946752106
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-ac87bae8-b5db-4f87-8787-a70937b10047-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ac87bae8-b5db-4f87-8787-a70937b10047
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636946752121
Started App in 1.343 seconds (JVM running for 3.259)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
null
java.lang.NullPointerException: null
	at java.util.Hashtable.putAll(Hashtable.java:523)
	at org.apache.flink.connector.kafka.source.KafkaSourceBuilder.setProperties(KafkaSourceBuilder.java:395)
	at com.yy.task.FlinkConsumerKafkaAppTask.run(FlinkConsumerKafkaAppTask.java:50)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:32)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:22)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 16636 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-d32b58f4-1759-4693-bf58-d3a1a873343e-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = d32b58f4-1759-4693-bf58-d3a1a873343e
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636947601738
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-141e6dc0-29c5-44fd-8161-9a9668e3722d-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 141e6dc0-29c5-44fd-8161-9a9668e3722d
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636947601750
Started App in 1.303 seconds (JVM running for 3.78)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
null
java.lang.NullPointerException: null
	at java.util.Hashtable.putAll(Hashtable.java:523)
	at org.apache.flink.connector.kafka.source.KafkaSourceBuilder.setProperties(KafkaSourceBuilder.java:395)
	at com.yy.task.FlinkConsumerKafkaAppTask.run(FlinkConsumerKafkaAppTask.java:50)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:32)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:22)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 3676 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
hello flink
null
java.lang.NullPointerException: null
	at java.util.Hashtable.putAll(Hashtable.java:523)
	at org.apache.flink.connector.kafka.source.KafkaSourceBuilder.setProperties(KafkaSourceBuilder.java:395)
	at com.yy.task.FlinkConsumerKafkaAppTask.run(FlinkConsumerKafkaAppTask.java:50)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:32)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:22)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-046212b5-e687-4fb8-ab63-617515ff7e39-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 046212b5-e687-4fb8-ab63-617515ff7e39
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636947667095
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-9f37094c-112a-4d26-96e5-d9e1eda39204-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 9f37094c-112a-4d26-96e5-d9e1eda39204
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636947667109
Started App in 1.36 seconds (JVM running for 3.355)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 11112 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
hello flink
null
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-37d0dcd7-c97a-4c79-8363-d3408de13952-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 37d0dcd7-c97a-4c79-8363-d3408de13952
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

null
java.lang.NullPointerException: null
	at java.util.Hashtable.putAll(Hashtable.java:523)
	at org.apache.flink.connector.kafka.source.KafkaSourceBuilder.setProperties(KafkaSourceBuilder.java:395)
	at com.yy.task.FlinkConsumerKafkaAppTask.run(FlinkConsumerKafkaAppTask.java:52)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:34)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:23)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636947860769
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-e36c4b72-5ce1-4a4a-b741-c0ef1b49c8f8-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = e36c4b72-5ce1-4a4a-b741-c0ef1b49c8f8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636947860783
Started App in 1.46 seconds (JVM running for 3.294)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 5820 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
hello flink
kafkaConf is null
java.lang.RuntimeException: kafkaConf is null
	at com.yy.task.FlinkConsumerKafkaAppTask.run(FlinkConsumerKafkaAppTask.java:52)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:34)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:23)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-0a202d5b-fa97-40e0-bedd-e1c086eb9ec8-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0a202d5b-fa97-40e0-bedd-e1c086eb9ec8
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636953804452
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-4dfcd848-bebb-4e83-a462-902e1eeee3d9-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 4dfcd848-bebb-4e83-a462-902e1eeee3d9
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636953804469
Started App in 1.536 seconds (JVM running for 3.755)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 17328 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
hello flink
kafkaConf is null
java.lang.RuntimeException: kafkaConf is null
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:34)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:23)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-de8f8b01-e393-4a6a-a01e-e9aacd648add-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = de8f8b01-e393-4a6a-a01e-e9aacd648add
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636954239578
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-ff9db445-9305-43b8-9e01-e74f5afcdcda-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = ff9db445-9305-43b8-9e01-e74f5afcdcda
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636954239595
Started App in 1.455 seconds (JVM running for 4.186)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 5564 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
hello flink
null
kafkaConf is null
java.lang.RuntimeException: kafkaConf is null
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:35)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:23)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-a4eb8133-7e3d-4b7a-a004-d5531e0bd977-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = a4eb8133-7e3d-4b7a-a004-d5531e0bd977
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636954378924
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-7c5127ed-d9b7-4a64-95fa-47c2c8387c6c-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 7c5127ed-d9b7-4a64-95fa-47c2c8387c6c
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636954378940
Started App in 1.365 seconds (JVM running for 3.156)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 14560 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
hello flink
null
kafkaConf is null
java.lang.RuntimeException: kafkaConf is null
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:35)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:23)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-6efa8d76-2cef-409f-be19-197045dba777-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6efa8d76-2cef-409f-be19-197045dba777
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636954559186
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-7de55e8b-fd4d-4ce8-8d43-c433d42807f0-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 7de55e8b-fd4d-4ce8-8d43-c433d42807f0
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636954559201
Started App in 6.3 seconds (JVM running for 8.085)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 10864 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'app': Invocation of init method failed; nested exception is java.lang.NullPointerException


Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.
Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'app': Invocation of init method failed; nested exception is java.lang.NullPointerException
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:160)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:422)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1778)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:602)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:524)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:944)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:918)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:438)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:337)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:144)
	at com.yy.App.main(App.java:27)
Caused by: java.lang.NullPointerException: null
	at com.yy.App.run(App.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleElement.invoke(InitDestroyAnnotationBeanPostProcessor.java:389)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:333)
	at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:157)
	... 16 common frames omitted
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 17328 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-1d9331ce-b89c-4061-9072-10f89296b7fc-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 1d9331ce-b89c-4061-9072-10f89296b7fc
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955019682
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-410ff49a-80d3-46e2-a6f7-624533be6915-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 410ff49a-80d3-46e2-a6f7-624533be6915
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955019695
Started App in 1.251 seconds (JVM running for 3.047)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 16392 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-0fed13bf-dc47-413e-b7d0-0653746a49ef-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0fed13bf-dc47-413e-b7d0-0653746a49ef
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955243118
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-7b555322-3b21-41a4-a774-0860be643ffa-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 7b555322-3b21-41a4-a774-0860be643ffa
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955243132
Started App in 1.294 seconds (JVM running for 2.978)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
kafkaConf is not null
Deserialization schema is required but not provided.
java.lang.NullPointerException: Deserialization schema is required but not provided.
	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:76)
	at org.apache.flink.connector.kafka.source.KafkaSourceBuilder.sanityCheck(KafkaSourceBuilder.java:487)
	at org.apache.flink.connector.kafka.source.KafkaSourceBuilder.build(KafkaSourceBuilder.java:405)
	at com.yy.task.FlinkConsumerKafkaAppTask.run(FlinkConsumerKafkaAppTask.java:59)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:39)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:23)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 17908 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-5476e55b-2d16-4002-80c3-97fb9980c6bc-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 5476e55b-2d16-4002-80c3-97fb9980c6bc
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955307090
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-fa9df739-bced-486b-b632-da3331379324-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = fa9df739-bced-486b-b632-da3331379324
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955307107
Started App in 1.26 seconds (JVM running for 3.029)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
{"key.deserializer":"org.apache.kafka.common.serialization.StringDeserializer","auto.offset.reset":"earliest","auto.commit.interval.ms":"1000","bootstrap.servers":"192.168.5.153:9092","enable.auto.commit":"true","group.id":"76bcf013-d871-41be-8923-95bfa0824342","deserializer.encoding":"utf-8","value.deserializer":"org.apache.kafka.common.serialization.StringDeserializer"}
kafkaConf is not null
Deserialization schema is required but not provided.
java.lang.NullPointerException: Deserialization schema is required but not provided.
	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:76)
	at org.apache.flink.connector.kafka.source.KafkaSourceBuilder.sanityCheck(KafkaSourceBuilder.java:487)
	at org.apache.flink.connector.kafka.source.KafkaSourceBuilder.build(KafkaSourceBuilder.java:405)
	at com.yy.task.FlinkConsumerKafkaAppTask.run(FlinkConsumerKafkaAppTask.java:60)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:40)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:23)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 11356 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-07c6efb7-c0eb-4165-b431-ab627104728e-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 07c6efb7-c0eb-4165-b431-ab627104728e
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955668766
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-b07dbd1b-1266-4181-ae9a-eaf3ec1c52fc-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b07dbd1b-1266-4181-ae9a-eaf3ec1c52fc
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955668781
Started App in 1.312 seconds (JVM running for 3.073)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
{"key.deserializer":"org.apache.kafka.common.serialization.StringDeserializer","auto.offset.reset":"earliest","auto.commit.interval.ms":"1000","bootstrap.servers":"192.168.5.153:9092","enable.auto.commit":"true","group.id":"d90451b7-3967-433f-bf4a-1d30b53fc224","deserializer.encoding":"utf-8","value.deserializer":"org.apache.kafka.common.serialization.StringDeserializer"}
kafkaConf is not null
Property key.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property value.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property auto.offset.reset is provided but will be overridden from earliest to earliest
The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
Starting Flink Mini Cluster
Starting Metrics Registry
No metrics reporter configured, no metrics will be exposed/reported.
Starting RPC Service(s)
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink-metrics
Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
Starting high-availability services
Created BLOB server storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-0809211e-add1-4e39-b38f-89ed331d88af
Started BLOB server at 0.0.0.0:7846 - max concurrent requests: 50 - max backlog: 1000
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-e33b90b8-540f-43e3-a13b-508064960398
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-c3cb2b68-bcdd-4f83-8f44-6126f0977bed
Starting 1 TaskManger(s)
Starting TaskManager with ResourceID: 07fdbf98-daec-4520-9004-c6b11c56418a
Temporary file directory 'C:\Users\MACHEN~1\AppData\Local\Temp': total 119 GB, usable 38 GB (31.93% usable)
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-io-29a317ee-720d-471f-9a4f-5b9bd0596276 for spill files.
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-netty-shuffle-fdf30761-7e5e-4bac-a1d1-73a89bab4812 for spill files.
Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
Starting the network environment and its components.
Starting the kvState service and its components.
Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
Start job leader service.
User file cache uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-dist-cache-d09e699a-9db3-411a-b5cf-adeacfa749c6
Upload directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-web-upload does not exist. 
Created directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-web-upload for file uploads.
Starting rest endpoint.
Log file environment variable 'log.file' is not set.
JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
Rest endpoint listening at localhost:7899
Proposing leadership to contender http://localhost:7899
Web frontend listening at http://localhost:7899.
http://localhost:7899 was granted leadership with leaderSessionID=44f18ef1-0554-4de4-8200-63cce0a4bb43
Received confirmation of leadership for leader http://localhost:7899 , session=44f18ef1-0554-4de4-8200-63cce0a4bb43
Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
Starting the resource manager.
Proposing leadership to contender LeaderContender: StandaloneResourceManager
ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token afccab37224ee41d9c9b50a9af2b49de
Flink Mini Cluster started successfully
Start SessionDispatcherLeaderProcess.
Recover all persisted job graphs.
Successfully recovered 0 persisted job graphs.
Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=9c9b50a9-af2b-49de-afcc-ab37224ee41d
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(afccab37224ee41d9c9b50a9af2b49de).
Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=f0ed7aa9-3534-4dd3-85d8-9e5aab6407a7
Resolved ResourceManager address, beginning registration
Registering TaskManager with ResourceID 07fdbf98-daec-4520-9004-c6b11c56418a (akka://flink/user/rpc/taskmanager_0) at ResourceManager
Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id e8f572e887d66cf7c181ec9620bb9615.
Received JobGraph submission 23da4dd95c50ba90ffe7978785bc5b72 (Flink Streaming Job).
Submitting job 23da4dd95c50ba90ffe7978785bc5b72 (Flink Streaming Job).
Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
Initializing job Flink Streaming Job (23da4dd95c50ba90ffe7978785bc5b72).
Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=3, backoffTimeMS=3000) for Flink Streaming Job (23da4dd95c50ba90ffe7978785bc5b72).
Running initialization on master for job Flink Streaming Job (23da4dd95c50ba90ffe7978785bc5b72).
Successfully ran initialization on master in 0 ms.
Built 6 pipelined regions in 1 ms
Using job/cluster config to configure application-defined state backend: EmbeddedRocksDBStateBackend{, localRocksDbDirectories=null, enableIncrementalCheckpointing=UNDEFINED, numberOfTransferThreads=-1, writeBatchSize=-1}
Using predefined options: DEFAULT.
Using default options factory: DefaultConfigurableOptionsFactory{configuredOptions={}}.
Using application-defined state backend: EmbeddedRocksDBStateBackend{, localRocksDbDirectories=null, enableIncrementalCheckpointing=FALSE, numberOfTransferThreads=4, writeBatchSize=2097152}
Using job/cluster config to configure application-defined checkpoint storage: org.apache.flink.runtime.state.storage.FileSystemCheckpointStorage@27cd47da
Job 23da4dd95c50ba90ffe7978785bc5b72 reached globally terminal state FAILED.
Shutting down Flink Mini Cluster
Stopping TaskExecutor akka://flink/user/rpc/taskmanager_0.
Close ResourceManager connection dbdeda2afccd05b367be11e58331296c.
Shutting down rest endpoint.
Closing TaskExecutor connection 07fdbf98-daec-4520-9004-c6b11c56418a because: The TaskExecutor is shutting down.
Stop job leader service.
Shutting down TaskExecutorLocalStateStoresManager.
FileChannelManager removed spill file directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-io-29a317ee-720d-471f-9a4f-5b9bd0596276
Shutting down the network environment and its components.
FileChannelManager removed spill file directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-netty-shuffle-fdf30761-7e5e-4bac-a1d1-73a89bab4812
Shutting down the kvState service and its components.
Stop job leader service.
Removing cache directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-web-ui
removed file cache directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-dist-cache-d09e699a-9db3-411a-b5cf-adeacfa749c6
Shut down complete.
Stopped TaskExecutor akka://flink/user/rpc/taskmanager_0.
Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..
Closing components.
Stopping SessionDispatcherLeaderProcess.
Stopping dispatcher akka://flink/user/rpc/dispatcher_2.
Stopping all currently running jobs of dispatcher akka://flink/user/rpc/dispatcher_2.
Closing the slot manager.
Suspending the slot manager.
Stopped dispatcher akka://flink/user/rpc/dispatcher_2.
Failed to execute job 'Flink Streaming Job'.
org.apache.flink.util.FlinkException: Failed to execute job 'Flink Streaming Job'.
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1970)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1848)
	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:69)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1834)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1817)
	at com.yy.task.FlinkConsumerKafkaAppTask.run(FlinkConsumerKafkaAppTask.java:74)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:41)
	at com.yy.task.FlinkConsumerKafkaAppTask.call(FlinkConsumerKafkaAppTask.java:24)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:316)
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:75)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:443)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
	at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1595)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.CompletionException: org.apache.flink.util.FlinkRuntimeException: Failed to create checkpoint storage at checkpoint coordinator side.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592)
	... 7 common frames omitted
Caused by: org.apache.flink.util.FlinkRuntimeException: Failed to create checkpoint storage at checkpoint coordinator side.
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.<init>(CheckpointCoordinator.java:324)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.<init>(CheckpointCoordinator.java:240)
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.enableCheckpointing(DefaultExecutionGraph.java:448)
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:311)
	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:107)
	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:342)
	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:190)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:120)
	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:132)
	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:110)
	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:340)
	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:317)
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:107)
	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
	... 7 common frames omitted
Caused by: java.io.IOException: Cannot instantiate file system for URI: hdfs://checkpoints/flink
	at org.apache.flink.runtime.fs.hdfs.HadoopFsFactory.create(HadoopFsFactory.java:196)
	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:526)
	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:407)
	at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274)
	at org.apache.flink.runtime.state.filesystem.FsCheckpointStorageAccess.<init>(FsCheckpointStorageAccess.java:64)
	at org.apache.flink.runtime.state.storage.FileSystemCheckpointStorage.createCheckpointStorage(FileSystemCheckpointStorage.java:323)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.<init>(CheckpointCoordinator.java:321)
	... 22 common frames omitted
Caused by: java.lang.IllegalArgumentException: java.net.UnknownHostException: checkpoints
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:445)
	at org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:140)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:355)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:289)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:172)
	at org.apache.flink.runtime.fs.hdfs.HadoopFsFactory.create(HadoopFsFactory.java:168)
	... 28 common frames omitted
Caused by: java.net.UnknownHostException: checkpoints
	... 34 common frames omitted
Stopping Akka RPC service.
Stopping Akka RPC service.
Stopped Akka RPC service.
Shutting down BLOB cache
Shutting down BLOB cache
Stopped BLOB server at 0.0.0.0:7846
Stopped Akka RPC service.
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 5112 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-0feb463d-8bf8-4b27-8356-5897dab87b1e-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 0feb463d-8bf8-4b27-8356-5897dab87b1e
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955798267
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-c3431e66-f056-448d-9e5b-717884eddbdc-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = c3431e66-f056-448d-9e5b-717884eddbdc
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955798282
Started App in 1.359 seconds (JVM running for 3.059)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
{"key.deserializer":"org.apache.kafka.common.serialization.StringDeserializer","auto.offset.reset":"earliest","auto.commit.interval.ms":"1000","bootstrap.servers":"192.168.5.153:9092","enable.auto.commit":"true","group.id":"6f5b45f2-8d90-461e-9aeb-07e550511d16","deserializer.encoding":"utf-8","value.deserializer":"org.apache.kafka.common.serialization.StringDeserializer"}
kafkaConf is not null
Property key.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property value.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property auto.offset.reset is provided but will be overridden from earliest to earliest
The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
Starting Flink Mini Cluster
Starting Metrics Registry
No metrics reporter configured, no metrics will be exposed/reported.
Starting RPC Service(s)
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink-metrics
Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
Starting high-availability services
Created BLOB server storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-d1019820-43aa-4a9b-a99a-89d0d9f4fb02
Started BLOB server at 0.0.0.0:8022 - max concurrent requests: 50 - max backlog: 1000
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-bc296e69-ee98-4282-a900-e3dc28a4a0b5
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-27914d35-255f-4dae-9699-e464656da690
Starting 1 TaskManger(s)
Starting TaskManager with ResourceID: 21736497-35eb-4f09-a35b-3ac121436f8d
Temporary file directory 'C:\Users\MACHEN~1\AppData\Local\Temp': total 119 GB, usable 38 GB (31.93% usable)
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-io-96f7607c-9f9a-4f90-833f-dde6624651c6 for spill files.
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-netty-shuffle-972f37b6-d792-4408-8771-561038217cc0 for spill files.
Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
Starting the network environment and its components.
Starting the kvState service and its components.
Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
Start job leader service.
User file cache uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-dist-cache-409bbac4-8549-4638-895d-d74d78b98721
Starting rest endpoint.
Log file environment variable 'log.file' is not set.
JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
Rest endpoint listening at localhost:8074
Proposing leadership to contender http://localhost:8074
Web frontend listening at http://localhost:8074.
http://localhost:8074 was granted leadership with leaderSessionID=50910f69-b7d5-40de-8508-9e87a49c488f
Received confirmation of leadership for leader http://localhost:8074 , session=50910f69-b7d5-40de-8508-9e87a49c488f
Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
Starting the resource manager.
Proposing leadership to contender LeaderContender: StandaloneResourceManager
ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 8a9e9a62c52e71bd61a4ac6e189b4a4d
Flink Mini Cluster started successfully
Start SessionDispatcherLeaderProcess.
Recover all persisted job graphs.
Successfully recovered 0 persisted job graphs.
Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=61a4ac6e-189b-4a4d-8a9e-9a62c52e71bd
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(8a9e9a62c52e71bd61a4ac6e189b4a4d).
Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=8592ab4f-52b3-4fce-a60b-892a80b3fcd5
Resolved ResourceManager address, beginning registration
Registering TaskManager with ResourceID 21736497-35eb-4f09-a35b-3ac121436f8d (akka://flink/user/rpc/taskmanager_0) at ResourceManager
Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 61a1fc0219c16311d5c5c42bc4f658bc.
Received JobGraph submission 41e8de6dc79c7f23424e537c37bcb866 (Flink Streaming Job).
Submitting job 41e8de6dc79c7f23424e537c37bcb866 (Flink Streaming Job).
Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
Initializing job Flink Streaming Job (41e8de6dc79c7f23424e537c37bcb866).
Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=3, backoffTimeMS=3000) for Flink Streaming Job (41e8de6dc79c7f23424e537c37bcb866).
Running initialization on master for job Flink Streaming Job (41e8de6dc79c7f23424e537c37bcb866).
Successfully ran initialization on master in 0 ms.
Built 6 pipelined regions in 1 ms
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
No checkpoint found during restore.
Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@eff03f8 for Flink Streaming Job (41e8de6dc79c7f23424e537c37bcb866).
Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=48ae61c3-6c8c-4ba2-985a-1aae5ff400be
Starting execution of job Flink Streaming Job (41e8de6dc79c7f23424e537c37bcb866) under job master id 985a1aae5ff400be48ae61c36c8c4ba2.
Starting split enumerator for source Source: kafka source -> Map -> Sink: Print to Std. Out.
Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
Job Flink Streaming Job (41e8de6dc79c7f23424e537c37bcb866) switched from state CREATED to RUNNING.
ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 6f5b45f2-8d90-461e-9aeb-07e550511d16-enumerator-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6f5b45f2-8d90-461e-9aeb-07e550511d16
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955801765
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (781e66a2ba5bfc8254c2f962e680fb5b) switched from CREATED to SCHEDULED.
AdminClientConfig values: 
	bootstrap.servers = [192.168.5.153:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 6f5b45f2-8d90-461e-9aeb-07e550511d16-enumerator-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

The configuration 'key.deserializer' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'value.deserializer' was supplied but isn't a known config.
The configuration 'enable.auto.commit' was supplied but isn't a known config.
The configuration 'group.id' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'auto.commit.interval.ms' was supplied but isn't a known config.
The configuration 'auto.offset.reset' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955801774
Starting the KafkaSourceEnumerator for consumer group 6f5b45f2-8d90-461e-9aeb-07e550511d16 without periodic partition discovery.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (50fb679908530023a5abb7ad8a32bc87) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (9ccc7730239d184e36b2caae32e99031) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (81192fc0a0bfacf94687927a047820f0) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (2e765f981e841ce287631f33756d433a) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (d2aa6fb5c87b97073579ff681926adf2) switched from CREATED to SCHEDULED.
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(8a9e9a62c52e71bd61a4ac6e189b4a4d)
Resolved ResourceManager address, beginning registration
Registering job manager 985a1aae5ff400be48ae61c36c8c4ba2@akka://flink/user/rpc/jobmanager_3 for job 41e8de6dc79c7f23424e537c37bcb866.
Registered job manager 985a1aae5ff400be48ae61c36c8c4ba2@akka://flink/user/rpc/jobmanager_3 for job 41e8de6dc79c7f23424e537c37bcb866.
JobManager successfully registered at ResourceManager, leader id: 8a9e9a62c52e71bd61a4ac6e189b4a4d.
Received resource requirements from job 41e8de6dc79c7f23424e537c37bcb866: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
Receive slot request 27fcfcf17470a3502f550a3ae8749677 for job 41e8de6dc79c7f23424e537c37bcb866 from resource manager with leader id 8a9e9a62c52e71bd61a4ac6e189b4a4d.
Allocated slot for 27fcfcf17470a3502f550a3ae8749677.
Add job 41e8de6dc79c7f23424e537c37bcb866 for job leader monitoring.
Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 48ae61c3-6c8c-4ba2-985a-1aae5ff400be.
Receive slot request 75d26049e631b5eef6292ceb9987d91b for job 41e8de6dc79c7f23424e537c37bcb866 from resource manager with leader id 8a9e9a62c52e71bd61a4ac6e189b4a4d.
Allocated slot for 75d26049e631b5eef6292ceb9987d91b.
Receive slot request 6dfa4502ea79867925573fcc83b56ad7 for job 41e8de6dc79c7f23424e537c37bcb866 from resource manager with leader id 8a9e9a62c52e71bd61a4ac6e189b4a4d.
Allocated slot for 6dfa4502ea79867925573fcc83b56ad7.
Receive slot request 845e7381f9ccb98134cfd974744e0446 for job 41e8de6dc79c7f23424e537c37bcb866 from resource manager with leader id 8a9e9a62c52e71bd61a4ac6e189b4a4d.
Allocated slot for 845e7381f9ccb98134cfd974744e0446.
Receive slot request ea23f020272485cc943edbc4fc605587 for job 41e8de6dc79c7f23424e537c37bcb866 from resource manager with leader id 8a9e9a62c52e71bd61a4ac6e189b4a4d.
Resolved JobManager address, beginning registration
Allocated slot for ea23f020272485cc943edbc4fc605587.
Receive slot request d10ada77bd07dc489596aedc6719f151 for job 41e8de6dc79c7f23424e537c37bcb866 from resource manager with leader id 8a9e9a62c52e71bd61a4ac6e189b4a4d.
Allocated slot for d10ada77bd07dc489596aedc6719f151.
Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 41e8de6dc79c7f23424e537c37bcb866.
Establish JobManager connection for job 41e8de6dc79c7f23424e537c37bcb866.
Offer reserved slots to the leader of job 41e8de6dc79c7f23424e537c37bcb866.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (781e66a2ba5bfc8254c2f962e680fb5b) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (attempt #0) with attempt id 781e66a2ba5bfc8254c2f962e680fb5b to 21736497-35eb-4f09-a35b-3ac121436f8d @ peer1 (dataPort=-1) with allocation id d10ada77bd07dc489596aedc6719f151
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (50fb679908530023a5abb7ad8a32bc87) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (attempt #0) with attempt id 50fb679908530023a5abb7ad8a32bc87 to 21736497-35eb-4f09-a35b-3ac121436f8d @ peer1 (dataPort=-1) with allocation id 27fcfcf17470a3502f550a3ae8749677
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (9ccc7730239d184e36b2caae32e99031) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (attempt #0) with attempt id 9ccc7730239d184e36b2caae32e99031 to 21736497-35eb-4f09-a35b-3ac121436f8d @ peer1 (dataPort=-1) with allocation id 6dfa4502ea79867925573fcc83b56ad7
Activate slot d10ada77bd07dc489596aedc6719f151.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (81192fc0a0bfacf94687927a047820f0) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (attempt #0) with attempt id 81192fc0a0bfacf94687927a047820f0 to 21736497-35eb-4f09-a35b-3ac121436f8d @ peer1 (dataPort=-1) with allocation id 845e7381f9ccb98134cfd974744e0446
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (2e765f981e841ce287631f33756d433a) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (attempt #0) with attempt id 2e765f981e841ce287631f33756d433a to 21736497-35eb-4f09-a35b-3ac121436f8d @ peer1 (dataPort=-1) with allocation id 75d26049e631b5eef6292ceb9987d91b
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (d2aa6fb5c87b97073579ff681926adf2) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (attempt #0) with attempt id d2aa6fb5c87b97073579ff681926adf2 to 21736497-35eb-4f09-a35b-3ac121436f8d @ peer1 (dataPort=-1) with allocation id ea23f020272485cc943edbc4fc605587
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (781e66a2ba5bfc8254c2f962e680fb5b), deploy into slot with allocation id d10ada77bd07dc489596aedc6719f151.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (781e66a2ba5bfc8254c2f962e680fb5b) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (781e66a2ba5bfc8254c2f962e680fb5b) [DEPLOYING].
Activate slot 27fcfcf17470a3502f550a3ae8749677.
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (50fb679908530023a5abb7ad8a32bc87), deploy into slot with allocation id 27fcfcf17470a3502f550a3ae8749677.
Activate slot 6dfa4502ea79867925573fcc83b56ad7.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (50fb679908530023a5abb7ad8a32bc87) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (50fb679908530023a5abb7ad8a32bc87) [DEPLOYING].
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (9ccc7730239d184e36b2caae32e99031), deploy into slot with allocation id 6dfa4502ea79867925573fcc83b56ad7.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (9ccc7730239d184e36b2caae32e99031) switched from CREATED to DEPLOYING.
Activate slot 845e7381f9ccb98134cfd974744e0446.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (9ccc7730239d184e36b2caae32e99031) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (81192fc0a0bfacf94687927a047820f0), deploy into slot with allocation id 845e7381f9ccb98134cfd974744e0446.
Activate slot 75d26049e631b5eef6292ceb9987d91b.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (81192fc0a0bfacf94687927a047820f0) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (81192fc0a0bfacf94687927a047820f0) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (2e765f981e841ce287631f33756d433a), deploy into slot with allocation id 75d26049e631b5eef6292ceb9987d91b.
Activate slot ea23f020272485cc943edbc4fc605587.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (2e765f981e841ce287631f33756d433a) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (2e765f981e841ce287631f33756d433a) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (d2aa6fb5c87b97073579ff681926adf2), deploy into slot with allocation id ea23f020272485cc943edbc4fc605587.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (81192fc0a0bfacf94687927a047820f0) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (781e66a2ba5bfc8254c2f962e680fb5b) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (d2aa6fb5c87b97073579ff681926adf2) switched from CREATED to DEPLOYING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (9ccc7730239d184e36b2caae32e99031) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (2e765f981e841ce287631f33756d433a) switched from DEPLOYING to INITIALIZING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (d2aa6fb5c87b97073579ff681926adf2) [DEPLOYING].
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (50fb679908530023a5abb7ad8a32bc87) switched from DEPLOYING to INITIALIZING.
Activate slot d10ada77bd07dc489596aedc6719f151.
Activate slot 27fcfcf17470a3502f550a3ae8749677.
Activate slot 6dfa4502ea79867925573fcc83b56ad7.
Activate slot 845e7381f9ccb98134cfd974744e0446.
Activate slot 75d26049e631b5eef6292ceb9987d91b.
Activate slot ea23f020272485cc943edbc4fc605587.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (781e66a2ba5bfc8254c2f962e680fb5b) switched from DEPLOYING to INITIALIZING.
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (d2aa6fb5c87b97073579ff681926adf2) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (9ccc7730239d184e36b2caae32e99031) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (81192fc0a0bfacf94687927a047820f0) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (2e765f981e841ce287631f33756d433a) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (50fb679908530023a5abb7ad8a32bc87) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (d2aa6fb5c87b97073579ff681926adf2) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (9ccc7730239d184e36b2caae32e99031) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (781e66a2ba5bfc8254c2f962e680fb5b) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (50fb679908530023a5abb7ad8a32bc87) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (81192fc0a0bfacf94687927a047820f0) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (d2aa6fb5c87b97073579ff681926adf2) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (2e765f981e841ce287631f33756d433a) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (9ccc7730239d184e36b2caae32e99031) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (781e66a2ba5bfc8254c2f962e680fb5b) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (50fb679908530023a5abb7ad8a32bc87) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (d2aa6fb5c87b97073579ff681926adf2) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (81192fc0a0bfacf94687927a047820f0) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (2e765f981e841ce287631f33756d433a) switched from INITIALIZING to RUNNING.
The following partitions have been added to the Kafka cluster. [flinkmsg-0, flinkmsg-2, flinkmsg-1]
Assigning splits to readers {0=[[Partition: flinkmsg-1, StartingOffset: -2, StoppingOffset: -9223372036854775808]], 1=[[Partition: flinkmsg-2, StartingOffset: -2, StoppingOffset: -9223372036854775808]], 5=[[Partition: flinkmsg-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
Adding split(s) to reader: [[Partition: flinkmsg-2, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Adding split(s) to reader: [[Partition: flinkmsg-1, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Adding split(s) to reader: [[Partition: flinkmsg-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 6f5b45f2-8d90-461e-9aeb-07e550511d16-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6f5b45f2-8d90-461e-9aeb-07e550511d16
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 6f5b45f2-8d90-461e-9aeb-07e550511d16-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6f5b45f2-8d90-461e-9aeb-07e550511d16
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 6f5b45f2-8d90-461e-9aeb-07e550511d16-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 6f5b45f2-8d90-461e-9aeb-07e550511d16
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

Closing Source Reader.
Closing Source Reader.
Closing Source Reader.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (2e765f981e841ce287631f33756d433a) switched from RUNNING to FINISHED.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (9ccc7730239d184e36b2caae32e99031) switched from RUNNING to FINISHED.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (81192fc0a0bfacf94687927a047820f0) switched from RUNNING to FINISHED.
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (9ccc7730239d184e36b2caae32e99031).
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (81192fc0a0bfacf94687927a047820f0).
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (2e765f981e841ce287631f33756d433a).
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 9ccc7730239d184e36b2caae32e99031.
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 2e765f981e841ce287631f33756d433a.
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 81192fc0a0bfacf94687927a047820f0.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (9ccc7730239d184e36b2caae32e99031) switched from RUNNING to FINISHED.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955802199
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
Kafka version: 2.6.1
The configuration 'client.id.prefix' was supplied but isn't a known config.
Kafka commitId: 6b2021cd52659cef
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka startTimeMs: 1636955802202
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636955802202
Received resource requirements from job 41e8de6dc79c7f23424e537c37bcb866: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (2e765f981e841ce287631f33756d433a) switched from RUNNING to FINISHED.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Received resource requirements from job 41e8de6dc79c7f23424e537c37bcb866: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
Reader received NoMoreSplits event.
Starting split fetcher 0
Starting split fetcher 0
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (81192fc0a0bfacf94687927a047820f0) switched from RUNNING to FINISHED.
Starting split fetcher 0
Received resource requirements from job 41e8de6dc79c7f23424e537c37bcb866: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-0, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Subscribed to partition(s): flinkmsg-1
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-1, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Subscribed to partition(s): flinkmsg-2
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-5, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Subscribed to partition(s): flinkmsg-0
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-0, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Seeking to EARLIEST offset of partition flinkmsg-1
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-5, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Seeking to EARLIEST offset of partition flinkmsg-0
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-1, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Seeking to EARLIEST offset of partition flinkmsg-2
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-0, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-1, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-5, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-0, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Resetting offset for partition flinkmsg-1 to offset 0.
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-5, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Resetting offset for partition flinkmsg-0 to offset 0.
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-1, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Resetting offset for partition flinkmsg-2 to offset 0.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-1, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Discovered group coordinator slave01:9092 (id: 2147483557 rack: null)
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-0, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Discovered group coordinator slave01:9092 (id: 2147483557 rack: null)
[Consumer clientId=6f5b45f2-8d90-461e-9aeb-07e550511d16-5, groupId=6f5b45f2-8d90-461e-9aeb-07e550511d16] Discovered group coordinator slave01:9092 (id: 2147483557 rack: null)
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 41e8de6dc79c7f23424e537c37bcb866 since some tasks of job 41e8de6dc79c7f23424e537c37bcb866 has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 2512 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-b128495e-8593-440b-9052-3f799f508cc2-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = b128495e-8593-440b-9052-3f799f508cc2
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956595908
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-81517184-a5a5-4600-b080-c5f74688a97b-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 81517184-a5a5-4600-b080-c5f74688a97b
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956595923
Started App in 1.342 seconds (JVM running for 3.17)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
{"key.deserializer":"org.apache.kafka.common.serialization.StringDeserializer","auto.offset.reset":"earliest","auto.commit.interval.ms":"1000","bootstrap.servers":"192.168.5.153:9092","enable.auto.commit":"true","group.id":"cfcd3947-1976-45d1-aef4-5d8ed0969d98","deserializer.encoding":"utf-8","value.deserializer":"org.apache.kafka.common.serialization.StringDeserializer"}
kafkaConf is not null
Property key.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property value.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property auto.offset.reset is provided but will be overridden from earliest to earliest
The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
Starting Flink Mini Cluster
Starting Metrics Registry
No metrics reporter configured, no metrics will be exposed/reported.
Starting RPC Service(s)
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink-metrics
Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
Starting high-availability services
Created BLOB server storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-e2f52302-4c45-44dc-b165-aaf9a37c384c
Started BLOB server at 0.0.0.0:8790 - max concurrent requests: 50 - max backlog: 1000
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-22b73b36-e062-43e2-954b-e518a27eebbe
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-8bbe1ae9-1bab-4f49-a75c-cf3fc229c18e
Starting 1 TaskManger(s)
Starting TaskManager with ResourceID: 5995917d-4c2e-4283-851c-52209f82ef73
Temporary file directory 'C:\Users\MACHEN~1\AppData\Local\Temp': total 119 GB, usable 38 GB (31.93% usable)
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-io-65e7d6fd-095a-46e6-8cd3-9e7acfba6ee7 for spill files.
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-netty-shuffle-14a295dc-4f01-4685-879c-8417410de15b for spill files.
Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
Starting the network environment and its components.
Starting the kvState service and its components.
Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
Start job leader service.
User file cache uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-dist-cache-8a74e6c4-1a59-48d8-8f54-3467f50beaa4
Starting rest endpoint.
Log file environment variable 'log.file' is not set.
JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
Rest endpoint listening at localhost:8843
Proposing leadership to contender http://localhost:8843
Web frontend listening at http://localhost:8843.
http://localhost:8843 was granted leadership with leaderSessionID=1e5356f8-c287-4758-9571-ce175f7f5cd2
Received confirmation of leadership for leader http://localhost:8843 , session=1e5356f8-c287-4758-9571-ce175f7f5cd2
Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
Starting the resource manager.
Proposing leadership to contender LeaderContender: StandaloneResourceManager
ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 9c2a56bbd16b55cc3a27d918c6d940aa
Flink Mini Cluster started successfully
Start SessionDispatcherLeaderProcess.
Recover all persisted job graphs.
Successfully recovered 0 persisted job graphs.
Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=3a27d918-c6d9-40aa-9c2a-56bbd16b55cc
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(9c2a56bbd16b55cc3a27d918c6d940aa).
Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=82d2e569-f2de-4679-b551-f73b6cab2fe0
Resolved ResourceManager address, beginning registration
Registering TaskManager with ResourceID 5995917d-4c2e-4283-851c-52209f82ef73 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 4efb5153aa99558cf438abc0d93e4499.
Received JobGraph submission 95987ab783ed515363ae87e1e866c6fd (Flink Streaming Job).
Submitting job 95987ab783ed515363ae87e1e866c6fd (Flink Streaming Job).
Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
Initializing job Flink Streaming Job (95987ab783ed515363ae87e1e866c6fd).
Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=3, backoffTimeMS=3000) for Flink Streaming Job (95987ab783ed515363ae87e1e866c6fd).
Running initialization on master for job Flink Streaming Job (95987ab783ed515363ae87e1e866c6fd).
Successfully ran initialization on master in 0 ms.
Built 6 pipelined regions in 1 ms
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
No checkpoint found during restore.
Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@10dadd67 for Flink Streaming Job (95987ab783ed515363ae87e1e866c6fd).
Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=2fb930e7-e8b9-402e-8cee-4263eb1fb403
Starting execution of job Flink Streaming Job (95987ab783ed515363ae87e1e866c6fd) under job master id 8cee4263eb1fb4032fb930e7e8b9402e.
Starting split enumerator for source Source: kafka source -> Map -> Sink: Print to Std. Out.
Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
Job Flink Streaming Job (95987ab783ed515363ae87e1e866c6fd) switched from state CREATED to RUNNING.
ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = cfcd3947-1976-45d1-aef4-5d8ed0969d98-enumerator-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = cfcd3947-1976-45d1-aef4-5d8ed0969d98
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

The configuration 'client.id.prefix' was supplied but isn't a known config.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (1abe960dcb9a071b1472e47243261945) switched from CREATED to SCHEDULED.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956599667
AdminClientConfig values: 
	bootstrap.servers = [192.168.5.153:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = cfcd3947-1976-45d1-aef4-5d8ed0969d98-enumerator-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

The configuration 'key.deserializer' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'value.deserializer' was supplied but isn't a known config.
The configuration 'enable.auto.commit' was supplied but isn't a known config.
The configuration 'group.id' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'auto.commit.interval.ms' was supplied but isn't a known config.
The configuration 'auto.offset.reset' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956599678
Starting the KafkaSourceEnumerator for consumer group cfcd3947-1976-45d1-aef4-5d8ed0969d98 without periodic partition discovery.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (ee64fd81aa6bd723b8af6a58c7b54906) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (7ee3591a7c10328054278a9251de291a) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (d98c04efddfd0cb0711597be171ffe7c) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (a2babd85b0df0f135e6881c2ffbd88df) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (c8dc560d3f534be1148c3fab7564f0bd) switched from CREATED to SCHEDULED.
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(9c2a56bbd16b55cc3a27d918c6d940aa)
Resolved ResourceManager address, beginning registration
Registering job manager 8cee4263eb1fb4032fb930e7e8b9402e@akka://flink/user/rpc/jobmanager_3 for job 95987ab783ed515363ae87e1e866c6fd.
Registered job manager 8cee4263eb1fb4032fb930e7e8b9402e@akka://flink/user/rpc/jobmanager_3 for job 95987ab783ed515363ae87e1e866c6fd.
JobManager successfully registered at ResourceManager, leader id: 9c2a56bbd16b55cc3a27d918c6d940aa.
Received resource requirements from job 95987ab783ed515363ae87e1e866c6fd: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
Receive slot request 1b0e4d94af3bd11b81097aafd3de3bea for job 95987ab783ed515363ae87e1e866c6fd from resource manager with leader id 9c2a56bbd16b55cc3a27d918c6d940aa.
Allocated slot for 1b0e4d94af3bd11b81097aafd3de3bea.
Add job 95987ab783ed515363ae87e1e866c6fd for job leader monitoring.
Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 2fb930e7-e8b9-402e-8cee-4263eb1fb403.
Receive slot request de074a9f04679a049faa57d954eabb32 for job 95987ab783ed515363ae87e1e866c6fd from resource manager with leader id 9c2a56bbd16b55cc3a27d918c6d940aa.
Allocated slot for de074a9f04679a049faa57d954eabb32.
Receive slot request 06fea9ba80bfc4a6963847162130bba1 for job 95987ab783ed515363ae87e1e866c6fd from resource manager with leader id 9c2a56bbd16b55cc3a27d918c6d940aa.
Allocated slot for 06fea9ba80bfc4a6963847162130bba1.
Receive slot request 497318fde4546e4c497eeef5847cb8e8 for job 95987ab783ed515363ae87e1e866c6fd from resource manager with leader id 9c2a56bbd16b55cc3a27d918c6d940aa.
Allocated slot for 497318fde4546e4c497eeef5847cb8e8.
Receive slot request df52fdea3e90a88ed65e959369ac5211 for job 95987ab783ed515363ae87e1e866c6fd from resource manager with leader id 9c2a56bbd16b55cc3a27d918c6d940aa.
Resolved JobManager address, beginning registration
Allocated slot for df52fdea3e90a88ed65e959369ac5211.
Receive slot request da1594395c40aac229cd66f3d4a71a1c for job 95987ab783ed515363ae87e1e866c6fd from resource manager with leader id 9c2a56bbd16b55cc3a27d918c6d940aa.
Allocated slot for da1594395c40aac229cd66f3d4a71a1c.
Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 95987ab783ed515363ae87e1e866c6fd.
Establish JobManager connection for job 95987ab783ed515363ae87e1e866c6fd.
Offer reserved slots to the leader of job 95987ab783ed515363ae87e1e866c6fd.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (1abe960dcb9a071b1472e47243261945) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (attempt #0) with attempt id 1abe960dcb9a071b1472e47243261945 to 5995917d-4c2e-4283-851c-52209f82ef73 @ peer1 (dataPort=-1) with allocation id 1b0e4d94af3bd11b81097aafd3de3bea
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (ee64fd81aa6bd723b8af6a58c7b54906) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (attempt #0) with attempt id ee64fd81aa6bd723b8af6a58c7b54906 to 5995917d-4c2e-4283-851c-52209f82ef73 @ peer1 (dataPort=-1) with allocation id de074a9f04679a049faa57d954eabb32
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (7ee3591a7c10328054278a9251de291a) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (attempt #0) with attempt id 7ee3591a7c10328054278a9251de291a to 5995917d-4c2e-4283-851c-52209f82ef73 @ peer1 (dataPort=-1) with allocation id 497318fde4546e4c497eeef5847cb8e8
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (d98c04efddfd0cb0711597be171ffe7c) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (attempt #0) with attempt id d98c04efddfd0cb0711597be171ffe7c to 5995917d-4c2e-4283-851c-52209f82ef73 @ peer1 (dataPort=-1) with allocation id 06fea9ba80bfc4a6963847162130bba1
Activate slot 1b0e4d94af3bd11b81097aafd3de3bea.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (a2babd85b0df0f135e6881c2ffbd88df) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (attempt #0) with attempt id a2babd85b0df0f135e6881c2ffbd88df to 5995917d-4c2e-4283-851c-52209f82ef73 @ peer1 (dataPort=-1) with allocation id da1594395c40aac229cd66f3d4a71a1c
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (c8dc560d3f534be1148c3fab7564f0bd) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (attempt #0) with attempt id c8dc560d3f534be1148c3fab7564f0bd to 5995917d-4c2e-4283-851c-52209f82ef73 @ peer1 (dataPort=-1) with allocation id df52fdea3e90a88ed65e959369ac5211
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (1abe960dcb9a071b1472e47243261945), deploy into slot with allocation id 1b0e4d94af3bd11b81097aafd3de3bea.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (1abe960dcb9a071b1472e47243261945) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (1abe960dcb9a071b1472e47243261945) [DEPLOYING].
Activate slot de074a9f04679a049faa57d954eabb32.
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (ee64fd81aa6bd723b8af6a58c7b54906), deploy into slot with allocation id de074a9f04679a049faa57d954eabb32.
Activate slot 497318fde4546e4c497eeef5847cb8e8.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (ee64fd81aa6bd723b8af6a58c7b54906) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (ee64fd81aa6bd723b8af6a58c7b54906) [DEPLOYING].
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since Checkpoint triggering task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) of job 95987ab783ed515363ae87e1e866c6fd has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (7ee3591a7c10328054278a9251de291a), deploy into slot with allocation id 497318fde4546e4c497eeef5847cb8e8.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (7ee3591a7c10328054278a9251de291a) switched from CREATED to DEPLOYING.
Activate slot 06fea9ba80bfc4a6963847162130bba1.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (7ee3591a7c10328054278a9251de291a) [DEPLOYING].
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d98c04efddfd0cb0711597be171ffe7c), deploy into slot with allocation id 06fea9ba80bfc4a6963847162130bba1.
Activate slot da1594395c40aac229cd66f3d4a71a1c.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d98c04efddfd0cb0711597be171ffe7c) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d98c04efddfd0cb0711597be171ffe7c) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (a2babd85b0df0f135e6881c2ffbd88df), deploy into slot with allocation id da1594395c40aac229cd66f3d4a71a1c.
Activate slot df52fdea3e90a88ed65e959369ac5211.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (a2babd85b0df0f135e6881c2ffbd88df) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (a2babd85b0df0f135e6881c2ffbd88df) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (c8dc560d3f534be1148c3fab7564f0bd), deploy into slot with allocation id df52fdea3e90a88ed65e959369ac5211.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (c8dc560d3f534be1148c3fab7564f0bd) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (c8dc560d3f534be1148c3fab7564f0bd) [DEPLOYING].
Activate slot 1b0e4d94af3bd11b81097aafd3de3bea.
Activate slot de074a9f04679a049faa57d954eabb32.
Activate slot 497318fde4546e4c497eeef5847cb8e8.
Activate slot 06fea9ba80bfc4a6963847162130bba1.
Activate slot da1594395c40aac229cd66f3d4a71a1c.
Activate slot df52fdea3e90a88ed65e959369ac5211.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (a2babd85b0df0f135e6881c2ffbd88df) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (ee64fd81aa6bd723b8af6a58c7b54906) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (7ee3591a7c10328054278a9251de291a) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (1abe960dcb9a071b1472e47243261945) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d98c04efddfd0cb0711597be171ffe7c) switched from DEPLOYING to INITIALIZING.
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (c8dc560d3f534be1148c3fab7564f0bd) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (7ee3591a7c10328054278a9251de291a) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (a2babd85b0df0f135e6881c2ffbd88df) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (ee64fd81aa6bd723b8af6a58c7b54906) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (1abe960dcb9a071b1472e47243261945) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (d98c04efddfd0cb0711597be171ffe7c) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (c8dc560d3f534be1148c3fab7564f0bd) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (1abe960dcb9a071b1472e47243261945) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (c8dc560d3f534be1148c3fab7564f0bd) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (ee64fd81aa6bd723b8af6a58c7b54906) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (a2babd85b0df0f135e6881c2ffbd88df) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d98c04efddfd0cb0711597be171ffe7c) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (7ee3591a7c10328054278a9251de291a) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (1abe960dcb9a071b1472e47243261945) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (c8dc560d3f534be1148c3fab7564f0bd) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (ee64fd81aa6bd723b8af6a58c7b54906) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (a2babd85b0df0f135e6881c2ffbd88df) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (d98c04efddfd0cb0711597be171ffe7c) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (7ee3591a7c10328054278a9251de291a) switched from INITIALIZING to RUNNING.
The following partitions have been added to the Kafka cluster. [flinkmsg-0, flinkmsg-2, flinkmsg-1]
Assigning splits to readers {0=[[Partition: flinkmsg-1, StartingOffset: -2, StoppingOffset: -9223372036854775808]], 1=[[Partition: flinkmsg-2, StartingOffset: -2, StoppingOffset: -9223372036854775808]], 5=[[Partition: flinkmsg-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
Adding split(s) to reader: [[Partition: flinkmsg-1, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Adding split(s) to reader: [[Partition: flinkmsg-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Adding split(s) to reader: [[Partition: flinkmsg-2, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = cfcd3947-1976-45d1-aef4-5d8ed0969d98-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = cfcd3947-1976-45d1-aef4-5d8ed0969d98
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = cfcd3947-1976-45d1-aef4-5d8ed0969d98-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = cfcd3947-1976-45d1-aef4-5d8ed0969d98
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = cfcd3947-1976-45d1-aef4-5d8ed0969d98-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = cfcd3947-1976-45d1-aef4-5d8ed0969d98
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

Closing Source Reader.
Closing Source Reader.
Closing Source Reader.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (a2babd85b0df0f135e6881c2ffbd88df) switched from RUNNING to FINISHED.
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (a2babd85b0df0f135e6881c2ffbd88df).
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (7ee3591a7c10328054278a9251de291a) switched from RUNNING to FINISHED.
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (7ee3591a7c10328054278a9251de291a).
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d98c04efddfd0cb0711597be171ffe7c) switched from RUNNING to FINISHED.
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d98c04efddfd0cb0711597be171ffe7c).
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 a2babd85b0df0f135e6881c2ffbd88df.
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 7ee3591a7c10328054278a9251de291a.
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 d98c04efddfd0cb0711597be171ffe7c.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (a2babd85b0df0f135e6881c2ffbd88df) switched from RUNNING to FINISHED.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka version: 2.6.1
The configuration 'client.id.prefix' was supplied but isn't a known config.
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956600137
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956600137
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956600137
Received resource requirements from job 95987ab783ed515363ae87e1e866c6fd: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (7ee3591a7c10328054278a9251de291a) switched from RUNNING to FINISHED.
Received resource requirements from job 95987ab783ed515363ae87e1e866c6fd: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (d98c04efddfd0cb0711597be171ffe7c) switched from RUNNING to FINISHED.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Starting split fetcher 0
Starting split fetcher 0
Starting split fetcher 0
Received resource requirements from job 95987ab783ed515363ae87e1e866c6fd: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-1, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Subscribed to partition(s): flinkmsg-2
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-5, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Subscribed to partition(s): flinkmsg-0
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-0, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Subscribed to partition(s): flinkmsg-1
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-5, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Seeking to EARLIEST offset of partition flinkmsg-0
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-0, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Seeking to EARLIEST offset of partition flinkmsg-1
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-1, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Seeking to EARLIEST offset of partition flinkmsg-2
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-0, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-1, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-5, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-1, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Resetting offset for partition flinkmsg-2 to offset 0.
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-5, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Resetting offset for partition flinkmsg-0 to offset 0.
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-0, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Resetting offset for partition flinkmsg-1 to offset 0.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-1, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Discovered group coordinator master01:9092 (id: 2147483567 rack: null)
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-0, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Discovered group coordinator master01:9092 (id: 2147483567 rack: null)
[Consumer clientId=cfcd3947-1976-45d1-aef4-5d8ed0969d98-5, groupId=cfcd3947-1976-45d1-aef4-5d8ed0969d98] Discovered group coordinator master01:9092 (id: 2147483567 rack: null)
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Releasing slot [06fea9ba80bfc4a6963847162130bba1].
Releasing slot [497318fde4546e4c497eeef5847cb8e8].
Releasing slot [da1594395c40aac229cd66f3d4a71a1c].
Free slot TaskSlot(index:2, state:ACTIVE, resource profile: ResourceProfile{taskHeapMemory=170.667gb (183251937962 bytes), taskOffHeapMemory=170.667gb (183251937962 bytes), managedMemory=21.333mb (22369621 bytes), networkMemory=10.667mb (11184810 bytes)}, allocationId: 06fea9ba80bfc4a6963847162130bba1, jobId: 95987ab783ed515363ae87e1e866c6fd).
Free slot TaskSlot(index:3, state:ACTIVE, resource profile: ResourceProfile{taskHeapMemory=170.667gb (183251937962 bytes), taskOffHeapMemory=170.667gb (183251937962 bytes), managedMemory=21.333mb (22369621 bytes), networkMemory=10.667mb (11184810 bytes)}, allocationId: 497318fde4546e4c497eeef5847cb8e8, jobId: 95987ab783ed515363ae87e1e866c6fd).
Free slot TaskSlot(index:5, state:ACTIVE, resource profile: ResourceProfile{taskHeapMemory=170.667gb (183251937962 bytes), taskOffHeapMemory=170.667gb (183251937962 bytes), managedMemory=21.333mb (22369621 bytes), networkMemory=10.667mb (11184810 bytes)}, allocationId: da1594395c40aac229cd66f3d4a71a1c, jobId: 95987ab783ed515363ae87e1e866c6fd).
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 95987ab783ed515363ae87e1e866c6fd since some tasks of job 95987ab783ed515363ae87e1e866c6fd has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 12536 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-9f3fd67f-6c94-4177-8a12-696fae864a3e-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 9f3fd67f-6c94-4177-8a12-696fae864a3e
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956760720
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-e3dff5b4-71cd-4be8-b349-97770c97ec5e-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = e3dff5b4-71cd-4be8-b349-97770c97ec5e
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956760737
Started App in 1.395 seconds (JVM running for 3.294)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
{"key.deserializer":"org.apache.kafka.common.serialization.StringDeserializer","auto.offset.reset":"earliest","auto.commit.interval.ms":"1000","bootstrap.servers":"192.168.5.153:9092","enable.auto.commit":"true","group.id":"756daf8e-4dbf-4017-b4a3-826d1f6b98d4","deserializer.encoding":"utf-8","value.deserializer":"org.apache.kafka.common.serialization.StringDeserializer"}
kafkaConf is not null
Property key.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property value.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property auto.offset.reset is provided but will be overridden from earliest to earliest
The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
Starting Flink Mini Cluster
Starting Metrics Registry
No metrics reporter configured, no metrics will be exposed/reported.
Starting RPC Service(s)
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink-metrics
Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
Starting high-availability services
Created BLOB server storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-61228a79-6b94-4cfc-bbcc-4c9338ea8b31
Started BLOB server at 0.0.0.0:9049 - max concurrent requests: 50 - max backlog: 1000
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-4ffadae7-5587-4d93-85a4-dc2b33942268
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-17ea78cf-6f41-45f5-b6cc-d207a7e39203
Starting 1 TaskManger(s)
Starting TaskManager with ResourceID: 6a950982-2a2f-49f3-8b2b-eaeda34bfb98
Temporary file directory 'C:\Users\MACHEN~1\AppData\Local\Temp': total 119 GB, usable 38 GB (31.93% usable)
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-io-5ea2a437-b1ce-4d12-a5d1-8ebddfa588c7 for spill files.
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-netty-shuffle-816522db-9bed-4706-895f-3fdd34a5c178 for spill files.
Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
Starting the network environment and its components.
Starting the kvState service and its components.
Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
Start job leader service.
User file cache uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-dist-cache-fa67c90c-be3d-49dc-92cb-bd02d3b757b2
Starting rest endpoint.
Log file environment variable 'log.file' is not set.
JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
Rest endpoint listening at localhost:9101
Proposing leadership to contender http://localhost:9101
Web frontend listening at http://localhost:9101.
http://localhost:9101 was granted leadership with leaderSessionID=c226f25a-cf82-4fdf-b1a5-e3c4e950e5d4
Received confirmation of leadership for leader http://localhost:9101 , session=c226f25a-cf82-4fdf-b1a5-e3c4e950e5d4
Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
Starting the resource manager.
Proposing leadership to contender LeaderContender: StandaloneResourceManager
ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token 95dd28c66c5aa196c58b9be25dd8474d
Flink Mini Cluster started successfully
Start SessionDispatcherLeaderProcess.
Recover all persisted job graphs.
Successfully recovered 0 persisted job graphs.
Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=c58b9be2-5dd8-474d-95dd-28c66c5aa196
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(95dd28c66c5aa196c58b9be25dd8474d).
Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=5dcc122e-e39d-4e54-9fd0-766868305550
Resolved ResourceManager address, beginning registration
Registering TaskManager with ResourceID 6a950982-2a2f-49f3-8b2b-eaeda34bfb98 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 892d1cb3485178cbc37e5d16392a6a1a.
Received JobGraph submission 4a5749559f0c71648df7b4ea72c45cdb (Flink Streaming Job).
Submitting job 4a5749559f0c71648df7b4ea72c45cdb (Flink Streaming Job).
Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
Initializing job Flink Streaming Job (4a5749559f0c71648df7b4ea72c45cdb).
Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=3, backoffTimeMS=3000) for Flink Streaming Job (4a5749559f0c71648df7b4ea72c45cdb).
Running initialization on master for job Flink Streaming Job (4a5749559f0c71648df7b4ea72c45cdb).
Successfully ran initialization on master in 0 ms.
Built 6 pipelined regions in 1 ms
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
No checkpoint found during restore.
Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@465e6d9e for Flink Streaming Job (4a5749559f0c71648df7b4ea72c45cdb).
Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=26462758-9483-4fae-be5b-89d7bc6c9595
Starting execution of job Flink Streaming Job (4a5749559f0c71648df7b4ea72c45cdb) under job master id be5b89d7bc6c95952646275894834fae.
Starting split enumerator for source Source: kafka source -> Map -> Sink: Print to Std. Out.
Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
Job Flink Streaming Job (4a5749559f0c71648df7b4ea72c45cdb) switched from state CREATED to RUNNING.
ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 756daf8e-4dbf-4017-b4a3-826d1f6b98d4-enumerator-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 756daf8e-4dbf-4017-b4a3-826d1f6b98d4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956764380
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (95bdf332adc3b1d4286934ac196a62d2) switched from CREATED to SCHEDULED.
AdminClientConfig values: 
	bootstrap.servers = [192.168.5.153:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 756daf8e-4dbf-4017-b4a3-826d1f6b98d4-enumerator-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

The configuration 'key.deserializer' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'value.deserializer' was supplied but isn't a known config.
The configuration 'enable.auto.commit' was supplied but isn't a known config.
The configuration 'group.id' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'auto.commit.interval.ms' was supplied but isn't a known config.
The configuration 'auto.offset.reset' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956764391
Starting the KafkaSourceEnumerator for consumer group 756daf8e-4dbf-4017-b4a3-826d1f6b98d4 without periodic partition discovery.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (025ba6bc330ccfaafa7283da45f8ac12) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (1e445afb9b6800394ef9d42226b396cc) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (577fd6b09dbaa3a44594a53703347179) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (3a9a0e8f1d261c8ea472da7f08828940) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (97aea842865430cc6eb59fad5caddbd2) switched from CREATED to SCHEDULED.
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(95dd28c66c5aa196c58b9be25dd8474d)
Resolved ResourceManager address, beginning registration
Registering job manager be5b89d7bc6c95952646275894834fae@akka://flink/user/rpc/jobmanager_3 for job 4a5749559f0c71648df7b4ea72c45cdb.
Registered job manager be5b89d7bc6c95952646275894834fae@akka://flink/user/rpc/jobmanager_3 for job 4a5749559f0c71648df7b4ea72c45cdb.
JobManager successfully registered at ResourceManager, leader id: 95dd28c66c5aa196c58b9be25dd8474d.
Received resource requirements from job 4a5749559f0c71648df7b4ea72c45cdb: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
Receive slot request 486f4ca27a408a2257c6c812c06fbda5 for job 4a5749559f0c71648df7b4ea72c45cdb from resource manager with leader id 95dd28c66c5aa196c58b9be25dd8474d.
Allocated slot for 486f4ca27a408a2257c6c812c06fbda5.
Add job 4a5749559f0c71648df7b4ea72c45cdb for job leader monitoring.
Receive slot request e40269cd171441322e3ef00523b59c69 for job 4a5749559f0c71648df7b4ea72c45cdb from resource manager with leader id 95dd28c66c5aa196c58b9be25dd8474d.
Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id 26462758-9483-4fae-be5b-89d7bc6c9595.
Allocated slot for e40269cd171441322e3ef00523b59c69.
Receive slot request 8ae361e015d2d243071986b24f919dcb for job 4a5749559f0c71648df7b4ea72c45cdb from resource manager with leader id 95dd28c66c5aa196c58b9be25dd8474d.
Allocated slot for 8ae361e015d2d243071986b24f919dcb.
Receive slot request d8fb602abae941fdf17bf012291ccc6f for job 4a5749559f0c71648df7b4ea72c45cdb from resource manager with leader id 95dd28c66c5aa196c58b9be25dd8474d.
Allocated slot for d8fb602abae941fdf17bf012291ccc6f.
Receive slot request ff75a679f226c973951fa395527bb9d0 for job 4a5749559f0c71648df7b4ea72c45cdb from resource manager with leader id 95dd28c66c5aa196c58b9be25dd8474d.
Allocated slot for ff75a679f226c973951fa395527bb9d0.
Receive slot request fe541098f6d16d4fbedf45472cb88a99 for job 4a5749559f0c71648df7b4ea72c45cdb from resource manager with leader id 95dd28c66c5aa196c58b9be25dd8474d.
Resolved JobManager address, beginning registration
Allocated slot for fe541098f6d16d4fbedf45472cb88a99.
Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 4a5749559f0c71648df7b4ea72c45cdb.
Establish JobManager connection for job 4a5749559f0c71648df7b4ea72c45cdb.
Offer reserved slots to the leader of job 4a5749559f0c71648df7b4ea72c45cdb.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (95bdf332adc3b1d4286934ac196a62d2) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (attempt #0) with attempt id 95bdf332adc3b1d4286934ac196a62d2 to 6a950982-2a2f-49f3-8b2b-eaeda34bfb98 @ peer1 (dataPort=-1) with allocation id ff75a679f226c973951fa395527bb9d0
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (025ba6bc330ccfaafa7283da45f8ac12) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (attempt #0) with attempt id 025ba6bc330ccfaafa7283da45f8ac12 to 6a950982-2a2f-49f3-8b2b-eaeda34bfb98 @ peer1 (dataPort=-1) with allocation id 486f4ca27a408a2257c6c812c06fbda5
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (1e445afb9b6800394ef9d42226b396cc) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (attempt #0) with attempt id 1e445afb9b6800394ef9d42226b396cc to 6a950982-2a2f-49f3-8b2b-eaeda34bfb98 @ peer1 (dataPort=-1) with allocation id 8ae361e015d2d243071986b24f919dcb
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (577fd6b09dbaa3a44594a53703347179) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (attempt #0) with attempt id 577fd6b09dbaa3a44594a53703347179 to 6a950982-2a2f-49f3-8b2b-eaeda34bfb98 @ peer1 (dataPort=-1) with allocation id fe541098f6d16d4fbedf45472cb88a99
Activate slot ff75a679f226c973951fa395527bb9d0.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (3a9a0e8f1d261c8ea472da7f08828940) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (attempt #0) with attempt id 3a9a0e8f1d261c8ea472da7f08828940 to 6a950982-2a2f-49f3-8b2b-eaeda34bfb98 @ peer1 (dataPort=-1) with allocation id e40269cd171441322e3ef00523b59c69
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (97aea842865430cc6eb59fad5caddbd2) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (attempt #0) with attempt id 97aea842865430cc6eb59fad5caddbd2 to 6a950982-2a2f-49f3-8b2b-eaeda34bfb98 @ peer1 (dataPort=-1) with allocation id d8fb602abae941fdf17bf012291ccc6f
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (95bdf332adc3b1d4286934ac196a62d2), deploy into slot with allocation id ff75a679f226c973951fa395527bb9d0.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (95bdf332adc3b1d4286934ac196a62d2) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (95bdf332adc3b1d4286934ac196a62d2) [DEPLOYING].
Activate slot 486f4ca27a408a2257c6c812c06fbda5.
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (025ba6bc330ccfaafa7283da45f8ac12), deploy into slot with allocation id 486f4ca27a408a2257c6c812c06fbda5.
Activate slot 8ae361e015d2d243071986b24f919dcb.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (025ba6bc330ccfaafa7283da45f8ac12) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (025ba6bc330ccfaafa7283da45f8ac12) [DEPLOYING].
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (1e445afb9b6800394ef9d42226b396cc), deploy into slot with allocation id 8ae361e015d2d243071986b24f919dcb.
Activate slot fe541098f6d16d4fbedf45472cb88a99.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (1e445afb9b6800394ef9d42226b396cc) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (1e445afb9b6800394ef9d42226b396cc) [DEPLOYING].
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (577fd6b09dbaa3a44594a53703347179), deploy into slot with allocation id fe541098f6d16d4fbedf45472cb88a99.
Activate slot e40269cd171441322e3ef00523b59c69.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (577fd6b09dbaa3a44594a53703347179) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (577fd6b09dbaa3a44594a53703347179) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3a9a0e8f1d261c8ea472da7f08828940), deploy into slot with allocation id e40269cd171441322e3ef00523b59c69.
Activate slot d8fb602abae941fdf17bf012291ccc6f.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3a9a0e8f1d261c8ea472da7f08828940) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3a9a0e8f1d261c8ea472da7f08828940) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (97aea842865430cc6eb59fad5caddbd2), deploy into slot with allocation id d8fb602abae941fdf17bf012291ccc6f.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (97aea842865430cc6eb59fad5caddbd2) switched from CREATED to DEPLOYING.
Activate slot ff75a679f226c973951fa395527bb9d0.
Activate slot 486f4ca27a408a2257c6c812c06fbda5.
Activate slot 8ae361e015d2d243071986b24f919dcb.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (97aea842865430cc6eb59fad5caddbd2) [DEPLOYING].
Activate slot fe541098f6d16d4fbedf45472cb88a99.
Activate slot e40269cd171441322e3ef00523b59c69.
Activate slot d8fb602abae941fdf17bf012291ccc6f.
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3a9a0e8f1d261c8ea472da7f08828940) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (95bdf332adc3b1d4286934ac196a62d2) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (1e445afb9b6800394ef9d42226b396cc) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (025ba6bc330ccfaafa7283da45f8ac12) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (577fd6b09dbaa3a44594a53703347179) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (97aea842865430cc6eb59fad5caddbd2) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (3a9a0e8f1d261c8ea472da7f08828940) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (95bdf332adc3b1d4286934ac196a62d2) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (1e445afb9b6800394ef9d42226b396cc) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (025ba6bc330ccfaafa7283da45f8ac12) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (577fd6b09dbaa3a44594a53703347179) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (97aea842865430cc6eb59fad5caddbd2) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (97aea842865430cc6eb59fad5caddbd2) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3a9a0e8f1d261c8ea472da7f08828940) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (025ba6bc330ccfaafa7283da45f8ac12) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (577fd6b09dbaa3a44594a53703347179) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (95bdf332adc3b1d4286934ac196a62d2) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (1e445afb9b6800394ef9d42226b396cc) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (3a9a0e8f1d261c8ea472da7f08828940) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (97aea842865430cc6eb59fad5caddbd2) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (025ba6bc330ccfaafa7283da45f8ac12) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (577fd6b09dbaa3a44594a53703347179) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (1e445afb9b6800394ef9d42226b396cc) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (95bdf332adc3b1d4286934ac196a62d2) switched from INITIALIZING to RUNNING.
The following partitions have been added to the Kafka cluster. [flinkmsg-0, flinkmsg-2, flinkmsg-1]
Assigning splits to readers {0=[[Partition: flinkmsg-1, StartingOffset: -2, StoppingOffset: -9223372036854775808]], 1=[[Partition: flinkmsg-2, StartingOffset: -2, StoppingOffset: -9223372036854775808]], 5=[[Partition: flinkmsg-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
Adding split(s) to reader: [[Partition: flinkmsg-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Adding split(s) to reader: [[Partition: flinkmsg-1, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Adding split(s) to reader: [[Partition: flinkmsg-2, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 756daf8e-4dbf-4017-b4a3-826d1f6b98d4-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 756daf8e-4dbf-4017-b4a3-826d1f6b98d4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 756daf8e-4dbf-4017-b4a3-826d1f6b98d4-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 756daf8e-4dbf-4017-b4a3-826d1f6b98d4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 756daf8e-4dbf-4017-b4a3-826d1f6b98d4-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 756daf8e-4dbf-4017-b4a3-826d1f6b98d4
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

Closing Source Reader.
Closing Source Reader.
Closing Source Reader.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (1e445afb9b6800394ef9d42226b396cc) switched from RUNNING to FINISHED.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (577fd6b09dbaa3a44594a53703347179) switched from RUNNING to FINISHED.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3a9a0e8f1d261c8ea472da7f08828940) switched from RUNNING to FINISHED.
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (577fd6b09dbaa3a44594a53703347179).
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3a9a0e8f1d261c8ea472da7f08828940).
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (1e445afb9b6800394ef9d42226b396cc).
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 577fd6b09dbaa3a44594a53703347179.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956764838
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 1e445afb9b6800394ef9d42226b396cc.
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 3a9a0e8f1d261c8ea472da7f08828940.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (577fd6b09dbaa3a44594a53703347179) switched from RUNNING to FINISHED.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka version: 2.6.1
The configuration 'deserializer.encoding' was supplied but isn't a known config.
Kafka commitId: 6b2021cd52659cef
The configuration 'client.id.prefix' was supplied but isn't a known config.
Kafka startTimeMs: 1636956764841
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636956764841
Received resource requirements from job 4a5749559f0c71648df7b4ea72c45cdb: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Starting split fetcher 0
Starting split fetcher 0
Starting split fetcher 0
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (1e445afb9b6800394ef9d42226b396cc) switched from RUNNING to FINISHED.
Received resource requirements from job 4a5749559f0c71648df7b4ea72c45cdb: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (3a9a0e8f1d261c8ea472da7f08828940) switched from RUNNING to FINISHED.
Received resource requirements from job 4a5749559f0c71648df7b4ea72c45cdb: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-5, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Subscribed to partition(s): flinkmsg-0
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-1, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Subscribed to partition(s): flinkmsg-2
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-0, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Subscribed to partition(s): flinkmsg-1
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-0, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Seeking to EARLIEST offset of partition flinkmsg-1
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-5, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Seeking to EARLIEST offset of partition flinkmsg-0
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-1, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Seeking to EARLIEST offset of partition flinkmsg-2
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-0, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-1, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-5, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-1, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Resetting offset for partition flinkmsg-2 to offset 0.
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-0, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Resetting offset for partition flinkmsg-1 to offset 0.
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-5, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Resetting offset for partition flinkmsg-0 to offset 0.
Failed to trigger checkpoint for job 4a5749559f0c71648df7b4ea72c45cdb since some tasks of job 4a5749559f0c71648df7b4ea72c45cdb has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-1, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Discovered group coordinator slave01:9092 (id: 2147483557 rack: null)
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-0, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Discovered group coordinator slave01:9092 (id: 2147483557 rack: null)
[Consumer clientId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4-5, groupId=756daf8e-4dbf-4017-b4a3-826d1f6b98d4] Discovered group coordinator slave01:9092 (id: 2147483557 rack: null)
Failed to trigger checkpoint for job 4a5749559f0c71648df7b4ea72c45cdb since some tasks of job 4a5749559f0c71648df7b4ea72c45cdb has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 4a5749559f0c71648df7b4ea72c45cdb since some tasks of job 4a5749559f0c71648df7b4ea72c45cdb has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 8884 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-76ad59ec-87ec-42fa-b9ae-cf513f0ee57f-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 76ad59ec-87ec-42fa-b9ae-cf513f0ee57f
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961411014
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-f3247e28-45a7-4d3b-9dbb-2087ba203fc3-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = f3247e28-45a7-4d3b-9dbb-2087ba203fc3
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961411028
Started App in 1.386 seconds (JVM running for 3.252)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
{"key.deserializer":"org.apache.kafka.common.serialization.StringDeserializer","auto.offset.reset":"earliest","auto.commit.interval.ms":"1000","bootstrap.servers":"192.168.5.153:9092","enable.auto.commit":"true","group.id":"099d1ff0-6390-41b3-ac3b-bb6561344b41","deserializer.encoding":"utf-8","value.deserializer":"org.apache.kafka.common.serialization.StringDeserializer"}
kafkaConf is not null
Property key.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property value.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property auto.offset.reset is provided but will be overridden from earliest to earliest
The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
Starting Flink Mini Cluster
Starting Metrics Registry
No metrics reporter configured, no metrics will be exposed/reported.
Starting RPC Service(s)
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink-metrics
Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
Starting high-availability services
Created BLOB server storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-179c871c-808a-4067-9e3f-d08d0807e122
Started BLOB server at 0.0.0.0:13772 - max concurrent requests: 50 - max backlog: 1000
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-c91a227b-1f59-4bf9-9cb7-94ca172369a6
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-bf4c66a7-d364-4621-9658-a224efa1327a
Starting 1 TaskManger(s)
Starting TaskManager with ResourceID: d1dab048-f8b1-4858-b821-41ec909282b9
Temporary file directory 'C:\Users\MACHEN~1\AppData\Local\Temp': total 119 GB, usable 31 GB (26.05% usable)
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-io-b63fdd73-0d9f-4082-8482-1712a4f7bac3 for spill files.
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-netty-shuffle-7ed55d3c-b5e7-4ab5-b87d-1d803141a080 for spill files.
Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
Starting the network environment and its components.
Starting the kvState service and its components.
Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
Start job leader service.
User file cache uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-dist-cache-d95db0a7-d921-476f-b548-9f422a8608ce
Starting rest endpoint.
Log file environment variable 'log.file' is not set.
JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
Rest endpoint listening at localhost:13824
Proposing leadership to contender http://localhost:13824
Web frontend listening at http://localhost:13824.
http://localhost:13824 was granted leadership with leaderSessionID=44237199-a15f-47a8-8b6b-94ac73f6fa79
Received confirmation of leadership for leader http://localhost:13824 , session=44237199-a15f-47a8-8b6b-94ac73f6fa79
Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
Starting the resource manager.
Proposing leadership to contender LeaderContender: StandaloneResourceManager
ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token b4fbb7d13235d348bddabbc6e83f48b1
Flink Mini Cluster started successfully
Start SessionDispatcherLeaderProcess.
Recover all persisted job graphs.
Successfully recovered 0 persisted job graphs.
Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=bddabbc6-e83f-48b1-b4fb-b7d13235d348
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(b4fbb7d13235d348bddabbc6e83f48b1).
Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=b14b928b-30e6-43f3-820a-04db62c898ea
Resolved ResourceManager address, beginning registration
Registering TaskManager with ResourceID d1dab048-f8b1-4858-b821-41ec909282b9 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 208e7c82978d91f1e23e1e0993583e97.
Received JobGraph submission eec9a4ba4e5d31ce32d11e1297946796 (Flink Streaming Job).
Submitting job eec9a4ba4e5d31ce32d11e1297946796 (Flink Streaming Job).
Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
Initializing job Flink Streaming Job (eec9a4ba4e5d31ce32d11e1297946796).
Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=3, backoffTimeMS=3000) for Flink Streaming Job (eec9a4ba4e5d31ce32d11e1297946796).
Running initialization on master for job Flink Streaming Job (eec9a4ba4e5d31ce32d11e1297946796).
Successfully ran initialization on master in 0 ms.
Built 6 pipelined regions in 1 ms
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
No checkpoint found during restore.
Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@6f3ae8b0 for Flink Streaming Job (eec9a4ba4e5d31ce32d11e1297946796).
Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=de3d8918-a620-4545-be33-33a76314efbe
Starting execution of job Flink Streaming Job (eec9a4ba4e5d31ce32d11e1297946796) under job master id be3333a76314efbede3d8918a6204545.
Starting split enumerator for source Source: kafka source -> Map -> Sink: Print to Std. Out.
Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
Job Flink Streaming Job (eec9a4ba4e5d31ce32d11e1297946796) switched from state CREATED to RUNNING.
ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 099d1ff0-6390-41b3-ac3b-bb6561344b41-enumerator-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 099d1ff0-6390-41b3-ac3b-bb6561344b41
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961414889
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (2a000b6f3d7d8e4160bcb02b94247ca0) switched from CREATED to SCHEDULED.
AdminClientConfig values: 
	bootstrap.servers = [192.168.5.153:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 099d1ff0-6390-41b3-ac3b-bb6561344b41-enumerator-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

The configuration 'key.deserializer' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'value.deserializer' was supplied but isn't a known config.
The configuration 'enable.auto.commit' was supplied but isn't a known config.
The configuration 'group.id' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'auto.commit.interval.ms' was supplied but isn't a known config.
The configuration 'auto.offset.reset' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961414903
Starting the KafkaSourceEnumerator for consumer group 099d1ff0-6390-41b3-ac3b-bb6561344b41 without periodic partition discovery.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (c3f5d90e076f55b4a4a9fc09385c2440) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (66e9fb691fed24d3e041fa7bad626b26) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (d0238ab63b4082370bb135d46814f294) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (e47bde077dfc99be802c9b36dfb2ee84) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (5e7134d01ced01c379b829c4fe000ffc) switched from CREATED to SCHEDULED.
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(b4fbb7d13235d348bddabbc6e83f48b1)
Resolved ResourceManager address, beginning registration
Registering job manager be3333a76314efbede3d8918a6204545@akka://flink/user/rpc/jobmanager_3 for job eec9a4ba4e5d31ce32d11e1297946796.
Registered job manager be3333a76314efbede3d8918a6204545@akka://flink/user/rpc/jobmanager_3 for job eec9a4ba4e5d31ce32d11e1297946796.
JobManager successfully registered at ResourceManager, leader id: b4fbb7d13235d348bddabbc6e83f48b1.
Received resource requirements from job eec9a4ba4e5d31ce32d11e1297946796: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
Receive slot request 67325cc55648c73bd9edddb3cc1b9469 for job eec9a4ba4e5d31ce32d11e1297946796 from resource manager with leader id b4fbb7d13235d348bddabbc6e83f48b1.
Allocated slot for 67325cc55648c73bd9edddb3cc1b9469.
Add job eec9a4ba4e5d31ce32d11e1297946796 for job leader monitoring.
Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id de3d8918-a620-4545-be33-33a76314efbe.
Receive slot request 18f601c2c4f3442d006664e72b05174b for job eec9a4ba4e5d31ce32d11e1297946796 from resource manager with leader id b4fbb7d13235d348bddabbc6e83f48b1.
Allocated slot for 18f601c2c4f3442d006664e72b05174b.
Receive slot request 9f1e82aadb1a85cbf5f1e2150bf3bd7a for job eec9a4ba4e5d31ce32d11e1297946796 from resource manager with leader id b4fbb7d13235d348bddabbc6e83f48b1.
Allocated slot for 9f1e82aadb1a85cbf5f1e2150bf3bd7a.
Receive slot request 2ebd0f149b0ab9c557ba3b2377d29171 for job eec9a4ba4e5d31ce32d11e1297946796 from resource manager with leader id b4fbb7d13235d348bddabbc6e83f48b1.
Allocated slot for 2ebd0f149b0ab9c557ba3b2377d29171.
Receive slot request ffd95afb00e37df7bdc98d927c5f798c for job eec9a4ba4e5d31ce32d11e1297946796 from resource manager with leader id b4fbb7d13235d348bddabbc6e83f48b1.
Allocated slot for ffd95afb00e37df7bdc98d927c5f798c.
Resolved JobManager address, beginning registration
Receive slot request 12f618bf154f6c227ca46227f8688cde for job eec9a4ba4e5d31ce32d11e1297946796 from resource manager with leader id b4fbb7d13235d348bddabbc6e83f48b1.
Allocated slot for 12f618bf154f6c227ca46227f8688cde.
Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job eec9a4ba4e5d31ce32d11e1297946796.
Establish JobManager connection for job eec9a4ba4e5d31ce32d11e1297946796.
Offer reserved slots to the leader of job eec9a4ba4e5d31ce32d11e1297946796.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (2a000b6f3d7d8e4160bcb02b94247ca0) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (attempt #0) with attempt id 2a000b6f3d7d8e4160bcb02b94247ca0 to d1dab048-f8b1-4858-b821-41ec909282b9 @ peer1 (dataPort=-1) with allocation id 67325cc55648c73bd9edddb3cc1b9469
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (c3f5d90e076f55b4a4a9fc09385c2440) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (attempt #0) with attempt id c3f5d90e076f55b4a4a9fc09385c2440 to d1dab048-f8b1-4858-b821-41ec909282b9 @ peer1 (dataPort=-1) with allocation id 9f1e82aadb1a85cbf5f1e2150bf3bd7a
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (66e9fb691fed24d3e041fa7bad626b26) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (attempt #0) with attempt id 66e9fb691fed24d3e041fa7bad626b26 to d1dab048-f8b1-4858-b821-41ec909282b9 @ peer1 (dataPort=-1) with allocation id 12f618bf154f6c227ca46227f8688cde
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (d0238ab63b4082370bb135d46814f294) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (attempt #0) with attempt id d0238ab63b4082370bb135d46814f294 to d1dab048-f8b1-4858-b821-41ec909282b9 @ peer1 (dataPort=-1) with allocation id 2ebd0f149b0ab9c557ba3b2377d29171
Activate slot 67325cc55648c73bd9edddb3cc1b9469.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (e47bde077dfc99be802c9b36dfb2ee84) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (attempt #0) with attempt id e47bde077dfc99be802c9b36dfb2ee84 to d1dab048-f8b1-4858-b821-41ec909282b9 @ peer1 (dataPort=-1) with allocation id 18f601c2c4f3442d006664e72b05174b
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (5e7134d01ced01c379b829c4fe000ffc) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (attempt #0) with attempt id 5e7134d01ced01c379b829c4fe000ffc to d1dab048-f8b1-4858-b821-41ec909282b9 @ peer1 (dataPort=-1) with allocation id ffd95afb00e37df7bdc98d927c5f798c
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (2a000b6f3d7d8e4160bcb02b94247ca0), deploy into slot with allocation id 67325cc55648c73bd9edddb3cc1b9469.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (2a000b6f3d7d8e4160bcb02b94247ca0) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (2a000b6f3d7d8e4160bcb02b94247ca0) [DEPLOYING].
Activate slot 9f1e82aadb1a85cbf5f1e2150bf3bd7a.
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (c3f5d90e076f55b4a4a9fc09385c2440), deploy into slot with allocation id 9f1e82aadb1a85cbf5f1e2150bf3bd7a.
Activate slot 12f618bf154f6c227ca46227f8688cde.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (c3f5d90e076f55b4a4a9fc09385c2440) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (c3f5d90e076f55b4a4a9fc09385c2440) [DEPLOYING].
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (66e9fb691fed24d3e041fa7bad626b26), deploy into slot with allocation id 12f618bf154f6c227ca46227f8688cde.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (66e9fb691fed24d3e041fa7bad626b26) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (66e9fb691fed24d3e041fa7bad626b26) [DEPLOYING].
Activate slot 2ebd0f149b0ab9c557ba3b2377d29171.
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d0238ab63b4082370bb135d46814f294), deploy into slot with allocation id 2ebd0f149b0ab9c557ba3b2377d29171.
Activate slot 18f601c2c4f3442d006664e72b05174b.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d0238ab63b4082370bb135d46814f294) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d0238ab63b4082370bb135d46814f294) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (e47bde077dfc99be802c9b36dfb2ee84), deploy into slot with allocation id 18f601c2c4f3442d006664e72b05174b.
Activate slot ffd95afb00e37df7bdc98d927c5f798c.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (e47bde077dfc99be802c9b36dfb2ee84) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (e47bde077dfc99be802c9b36dfb2ee84) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5e7134d01ced01c379b829c4fe000ffc), deploy into slot with allocation id ffd95afb00e37df7bdc98d927c5f798c.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5e7134d01ced01c379b829c4fe000ffc) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5e7134d01ced01c379b829c4fe000ffc) [DEPLOYING].
Activate slot 67325cc55648c73bd9edddb3cc1b9469.
Activate slot 9f1e82aadb1a85cbf5f1e2150bf3bd7a.
Activate slot 12f618bf154f6c227ca46227f8688cde.
Activate slot 2ebd0f149b0ab9c557ba3b2377d29171.
Activate slot 18f601c2c4f3442d006664e72b05174b.
Activate slot ffd95afb00e37df7bdc98d927c5f798c.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (e47bde077dfc99be802c9b36dfb2ee84) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (c3f5d90e076f55b4a4a9fc09385c2440) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (2a000b6f3d7d8e4160bcb02b94247ca0) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d0238ab63b4082370bb135d46814f294) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (66e9fb691fed24d3e041fa7bad626b26) switched from DEPLOYING to INITIALIZING.
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5e7134d01ced01c379b829c4fe000ffc) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (e47bde077dfc99be802c9b36dfb2ee84) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (c3f5d90e076f55b4a4a9fc09385c2440) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (2a000b6f3d7d8e4160bcb02b94247ca0) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (d0238ab63b4082370bb135d46814f294) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (66e9fb691fed24d3e041fa7bad626b26) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (5e7134d01ced01c379b829c4fe000ffc) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (66e9fb691fed24d3e041fa7bad626b26) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5e7134d01ced01c379b829c4fe000ffc) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (e47bde077dfc99be802c9b36dfb2ee84) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (c3f5d90e076f55b4a4a9fc09385c2440) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d0238ab63b4082370bb135d46814f294) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (2a000b6f3d7d8e4160bcb02b94247ca0) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (5e7134d01ced01c379b829c4fe000ffc) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (66e9fb691fed24d3e041fa7bad626b26) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (e47bde077dfc99be802c9b36dfb2ee84) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (c3f5d90e076f55b4a4a9fc09385c2440) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (d0238ab63b4082370bb135d46814f294) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (2a000b6f3d7d8e4160bcb02b94247ca0) switched from INITIALIZING to RUNNING.
The following partitions have been added to the Kafka cluster. [flinkmsg-0, flinkmsg-2, flinkmsg-1]
Assigning splits to readers {0=[[Partition: flinkmsg-1, StartingOffset: -2, StoppingOffset: -9223372036854775808]], 1=[[Partition: flinkmsg-2, StartingOffset: -2, StoppingOffset: -9223372036854775808]], 5=[[Partition: flinkmsg-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
Adding split(s) to reader: [[Partition: flinkmsg-1, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Adding split(s) to reader: [[Partition: flinkmsg-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Adding split(s) to reader: [[Partition: flinkmsg-2, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 099d1ff0-6390-41b3-ac3b-bb6561344b41-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 099d1ff0-6390-41b3-ac3b-bb6561344b41
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 099d1ff0-6390-41b3-ac3b-bb6561344b41-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 099d1ff0-6390-41b3-ac3b-bb6561344b41
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 099d1ff0-6390-41b3-ac3b-bb6561344b41-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 099d1ff0-6390-41b3-ac3b-bb6561344b41
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

Closing Source Reader.
Closing Source Reader.
Closing Source Reader.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (e47bde077dfc99be802c9b36dfb2ee84) switched from RUNNING to FINISHED.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (66e9fb691fed24d3e041fa7bad626b26) switched from RUNNING to FINISHED.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d0238ab63b4082370bb135d46814f294) switched from RUNNING to FINISHED.
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (66e9fb691fed24d3e041fa7bad626b26).
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (e47bde077dfc99be802c9b36dfb2ee84).
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (d0238ab63b4082370bb135d46814f294).
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 66e9fb691fed24d3e041fa7bad626b26.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961415350
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 d0238ab63b4082370bb135d46814f294.
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 e47bde077dfc99be802c9b36dfb2ee84.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (66e9fb691fed24d3e041fa7bad626b26) switched from RUNNING to FINISHED.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
Kafka version: 2.6.1
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961415352
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961415353
Reader received NoMoreSplits event.
Received resource requirements from job eec9a4ba4e5d31ce32d11e1297946796: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Starting split fetcher 0
Starting split fetcher 0
Starting split fetcher 0
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (d0238ab63b4082370bb135d46814f294) switched from RUNNING to FINISHED.
Received resource requirements from job eec9a4ba4e5d31ce32d11e1297946796: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-0, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Subscribed to partition(s): flinkmsg-1
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (e47bde077dfc99be802c9b36dfb2ee84) switched from RUNNING to FINISHED.
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-1, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Subscribed to partition(s): flinkmsg-2
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-5, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Subscribed to partition(s): flinkmsg-0
Received resource requirements from job eec9a4ba4e5d31ce32d11e1297946796: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-1, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Seeking to EARLIEST offset of partition flinkmsg-2
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-5, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Seeking to EARLIEST offset of partition flinkmsg-0
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-0, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Seeking to EARLIEST offset of partition flinkmsg-1
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-1, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-5, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-0, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-0, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Resetting offset for partition flinkmsg-1 to offset 0.
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-1, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Resetting offset for partition flinkmsg-2 to offset 0.
[Consumer clientId=099d1ff0-6390-41b3-ac3b-bb6561344b41-5, groupId=099d1ff0-6390-41b3-ac3b-bb6561344b41] Resetting offset for partition flinkmsg-0 to offset 0.
Starting App using Java 1.8.0_151 on DESKTOP-AGBUJPR with PID 14868 (D:\IdeaProject\gmail\flink_gmail\target\classes started by Machenike in D:\IdeaProject\gmail)
No active profile set, falling back to default profiles: default
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-8f25f06a-23cf-4318-acb6-70a6c94a9fdd-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 8f25f06a-23cf-4318-acb6-70a6c94a9fdd
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961454156
KafkaConfig(bootstrapServers=192.168.5.153:9092, keyDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, valueDeSerializer=org.apache.kafka.common.serialization.StringDeserializer, groupId=group1, enableAutoCommit=true, autoCommitInterval=1000, autoOffsetReset=earliest, deSerializerEncoding=utf-8)
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-5d3c62e2-f380-4ec5-9dbb-0f8723170e47-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 5d3c62e2-f380-4ec5-9dbb-0f8723170e47
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961454171
Started App in 1.299 seconds (JVM running for 3.05)
Application availability state LivenessState changed to CORRECT
Application availability state ReadinessState changed to ACCEPTING_TRAFFIC
hello flink
{"key.deserializer":"org.apache.kafka.common.serialization.StringDeserializer","auto.offset.reset":"earliest","auto.commit.interval.ms":"1000","bootstrap.servers":"192.168.5.153:9092","enable.auto.commit":"true","group.id":"2afef2cc-e80b-49cf-9be5-673e00778bef","deserializer.encoding":"utf-8","value.deserializer":"org.apache.kafka.common.serialization.StringDeserializer"}
kafkaConf is not null
Property key.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property value.deserializer is provided but will be overridden from org.apache.kafka.common.serialization.StringDeserializer to org.apache.kafka.common.serialization.ByteArrayDeserializer
Property auto.offset.reset is provided but will be overridden from earliest to earliest
The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
Starting Flink Mini Cluster
Starting Metrics Registry
No metrics reporter configured, no metrics will be exposed/reported.
Starting RPC Service(s)
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink
Trying to start local actor system
Slf4jLogger started
Actor system started at akka://flink-metrics
Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
Starting high-availability services
Created BLOB server storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-eba614f1-141a-4a95-aeea-f200f8923e58
Started BLOB server at 0.0.0.0:13910 - max concurrent requests: 50 - max backlog: 1000
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-c0859c8a-b441-4745-9654-3b67ff9decb4
Created BLOB cache storage directory C:\Users\MACHEN~1\AppData\Local\Temp\blobStore-8f833a06-fcc5-43c5-84fb-e1b77c9fcf9d
Starting 1 TaskManger(s)
Starting TaskManager with ResourceID: e892c524-e771-4aba-9891-abf18a1370f3
Temporary file directory 'C:\Users\MACHEN~1\AppData\Local\Temp': total 119 GB, usable 31 GB (26.05% usable)
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-io-436c769b-cc5b-4ce0-8fe1-9f0b152b8f7f for spill files.
FileChannelManager uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-netty-shuffle-ba7f9958-7e1c-45e2-b1b7-5e2ec2b20936 for spill files.
Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
Starting the network environment and its components.
Starting the kvState service and its components.
Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
Start job leader service.
User file cache uses directory C:\Users\MACHEN~1\AppData\Local\Temp\flink-dist-cache-5bfb5ddb-71e1-4f3d-b93f-26043d755dd3
Starting rest endpoint.
Log file environment variable 'log.file' is not set.
JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
Rest endpoint listening at localhost:13961
Proposing leadership to contender http://localhost:13961
Web frontend listening at http://localhost:13961.
http://localhost:13961 was granted leadership with leaderSessionID=a398e9b6-3631-4905-8652-46612e97ca22
Received confirmation of leadership for leader http://localhost:13961 , session=a398e9b6-3631-4905-8652-46612e97ca22
Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
Starting the resource manager.
Proposing leadership to contender LeaderContender: StandaloneResourceManager
ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token b17f42aa324c105d7724270131804608
Flink Mini Cluster started successfully
Start SessionDispatcherLeaderProcess.
Recover all persisted job graphs.
Successfully recovered 0 persisted job graphs.
Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=77242701-3180-4608-b17f-42aa324c105d
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(b17f42aa324c105d7724270131804608).
Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=74a08f45-69c4-46ec-98f9-4f9c93e002aa
Resolved ResourceManager address, beginning registration
Registering TaskManager with ResourceID e892c524-e771-4aba-9891-abf18a1370f3 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id 801250d11a4bc1367bb9598f1fdf82b9.
Received JobGraph submission 13a73f6a92cbf28074cfcfa7f707035e (Flink Streaming Job).
Submitting job 13a73f6a92cbf28074cfcfa7f707035e (Flink Streaming Job).
Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
Initializing job Flink Streaming Job (13a73f6a92cbf28074cfcfa7f707035e).
Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=3, backoffTimeMS=3000) for Flink Streaming Job (13a73f6a92cbf28074cfcfa7f707035e).
Running initialization on master for job Flink Streaming Job (13a73f6a92cbf28074cfcfa7f707035e).
Successfully ran initialization on master in 0 ms.
Built 6 pipelined regions in 1 ms
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
No checkpoint found during restore.
Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@322a1d81 for Flink Streaming Job (13a73f6a92cbf28074cfcfa7f707035e).
Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=d22816fd-c52e-45e5-8caa-a2d3a7b00dbe
Starting execution of job Flink Streaming Job (13a73f6a92cbf28074cfcfa7f707035e) under job master id 8caaa2d3a7b00dbed22816fdc52e45e5.
Starting split enumerator for source Source: kafka source -> Map -> Sink: Print to Std. Out.
Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
Job Flink Streaming Job (13a73f6a92cbf28074cfcfa7f707035e) switched from state CREATED to RUNNING.
ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 2afef2cc-e80b-49cf-9be5-673e00778bef-enumerator-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2afef2cc-e80b-49cf-9be5-673e00778bef
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (0e52a8b7fde2ea1d7ae70facb94e4d27) switched from CREATED to SCHEDULED.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961457800
AdminClientConfig values: 
	bootstrap.servers = [192.168.5.153:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 2afef2cc-e80b-49cf-9be5-673e00778bef-enumerator-admin-client
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

The configuration 'key.deserializer' was supplied but isn't a known config.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'value.deserializer' was supplied but isn't a known config.
The configuration 'enable.auto.commit' was supplied but isn't a known config.
The configuration 'group.id' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
The configuration 'auto.commit.interval.ms' was supplied but isn't a known config.
The configuration 'auto.offset.reset' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961457810
Starting the KafkaSourceEnumerator for consumer group 2afef2cc-e80b-49cf-9be5-673e00778bef without periodic partition discovery.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (fd62ab53c3b574531f76e7764ee7b5b2) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (dd8bd296344e4011f0eee3a374a6f70c) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (e95729b84aad8afdbdf6538b03b16d3b) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (3e1b18764dcdc88f0fa4a340ab9e9ec5) switched from CREATED to SCHEDULED.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (5ea13f14c09de7c28ba0c13cda57db87) switched from CREATED to SCHEDULED.
Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(b17f42aa324c105d7724270131804608)
Resolved ResourceManager address, beginning registration
Registering job manager 8caaa2d3a7b00dbed22816fdc52e45e5@akka://flink/user/rpc/jobmanager_3 for job 13a73f6a92cbf28074cfcfa7f707035e.
Registered job manager 8caaa2d3a7b00dbed22816fdc52e45e5@akka://flink/user/rpc/jobmanager_3 for job 13a73f6a92cbf28074cfcfa7f707035e.
JobManager successfully registered at ResourceManager, leader id: b17f42aa324c105d7724270131804608.
Received resource requirements from job 13a73f6a92cbf28074cfcfa7f707035e: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=6}]
Receive slot request 3b08d72bbe5e62246c08f241ea10292b for job 13a73f6a92cbf28074cfcfa7f707035e from resource manager with leader id b17f42aa324c105d7724270131804608.
Allocated slot for 3b08d72bbe5e62246c08f241ea10292b.
Add job 13a73f6a92cbf28074cfcfa7f707035e for job leader monitoring.
Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id d22816fd-c52e-45e5-8caa-a2d3a7b00dbe.
Receive slot request 64da75a54b0f87824e6a8c5001252f87 for job 13a73f6a92cbf28074cfcfa7f707035e from resource manager with leader id b17f42aa324c105d7724270131804608.
Allocated slot for 64da75a54b0f87824e6a8c5001252f87.
Receive slot request dd38a788566223d839535c438ad66311 for job 13a73f6a92cbf28074cfcfa7f707035e from resource manager with leader id b17f42aa324c105d7724270131804608.
Allocated slot for dd38a788566223d839535c438ad66311.
Receive slot request 51d153be151e91f4224db1d9e4f09599 for job 13a73f6a92cbf28074cfcfa7f707035e from resource manager with leader id b17f42aa324c105d7724270131804608.
Allocated slot for 51d153be151e91f4224db1d9e4f09599.
Receive slot request 80c9f83e4f787e5cad6f4b52932e4599 for job 13a73f6a92cbf28074cfcfa7f707035e from resource manager with leader id b17f42aa324c105d7724270131804608.
Allocated slot for 80c9f83e4f787e5cad6f4b52932e4599.
Resolved JobManager address, beginning registration
Receive slot request 0921a68b65560381fae70f98eafc3786 for job 13a73f6a92cbf28074cfcfa7f707035e from resource manager with leader id b17f42aa324c105d7724270131804608.
Allocated slot for 0921a68b65560381fae70f98eafc3786.
Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 13a73f6a92cbf28074cfcfa7f707035e.
Establish JobManager connection for job 13a73f6a92cbf28074cfcfa7f707035e.
Offer reserved slots to the leader of job 13a73f6a92cbf28074cfcfa7f707035e.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (0e52a8b7fde2ea1d7ae70facb94e4d27) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (attempt #0) with attempt id 0e52a8b7fde2ea1d7ae70facb94e4d27 to e892c524-e771-4aba-9891-abf18a1370f3 @ peer1 (dataPort=-1) with allocation id 51d153be151e91f4224db1d9e4f09599
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (fd62ab53c3b574531f76e7764ee7b5b2) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (attempt #0) with attempt id fd62ab53c3b574531f76e7764ee7b5b2 to e892c524-e771-4aba-9891-abf18a1370f3 @ peer1 (dataPort=-1) with allocation id 0921a68b65560381fae70f98eafc3786
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (dd8bd296344e4011f0eee3a374a6f70c) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (attempt #0) with attempt id dd8bd296344e4011f0eee3a374a6f70c to e892c524-e771-4aba-9891-abf18a1370f3 @ peer1 (dataPort=-1) with allocation id 80c9f83e4f787e5cad6f4b52932e4599
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (e95729b84aad8afdbdf6538b03b16d3b) switched from SCHEDULED to DEPLOYING.
Activate slot 51d153be151e91f4224db1d9e4f09599.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (attempt #0) with attempt id e95729b84aad8afdbdf6538b03b16d3b to e892c524-e771-4aba-9891-abf18a1370f3 @ peer1 (dataPort=-1) with allocation id 64da75a54b0f87824e6a8c5001252f87
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (3e1b18764dcdc88f0fa4a340ab9e9ec5) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (attempt #0) with attempt id 3e1b18764dcdc88f0fa4a340ab9e9ec5 to e892c524-e771-4aba-9891-abf18a1370f3 @ peer1 (dataPort=-1) with allocation id 3b08d72bbe5e62246c08f241ea10292b
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (5ea13f14c09de7c28ba0c13cda57db87) switched from SCHEDULED to DEPLOYING.
Deploying Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (attempt #0) with attempt id 5ea13f14c09de7c28ba0c13cda57db87 to e892c524-e771-4aba-9891-abf18a1370f3 @ peer1 (dataPort=-1) with allocation id dd38a788566223d839535c438ad66311
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (0e52a8b7fde2ea1d7ae70facb94e4d27), deploy into slot with allocation id 51d153be151e91f4224db1d9e4f09599.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (0e52a8b7fde2ea1d7ae70facb94e4d27) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (0e52a8b7fde2ea1d7ae70facb94e4d27) [DEPLOYING].
Activate slot 0921a68b65560381fae70f98eafc3786.
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (fd62ab53c3b574531f76e7764ee7b5b2), deploy into slot with allocation id 0921a68b65560381fae70f98eafc3786.
Activate slot 80c9f83e4f787e5cad6f4b52932e4599.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (fd62ab53c3b574531f76e7764ee7b5b2) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (fd62ab53c3b574531f76e7764ee7b5b2) [DEPLOYING].
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (dd8bd296344e4011f0eee3a374a6f70c), deploy into slot with allocation id 80c9f83e4f787e5cad6f4b52932e4599.
Activate slot 64da75a54b0f87824e6a8c5001252f87.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (dd8bd296344e4011f0eee3a374a6f70c) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (dd8bd296344e4011f0eee3a374a6f70c) [DEPLOYING].
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (e95729b84aad8afdbdf6538b03b16d3b), deploy into slot with allocation id 64da75a54b0f87824e6a8c5001252f87.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (e95729b84aad8afdbdf6538b03b16d3b) switched from CREATED to DEPLOYING.
Activate slot 3b08d72bbe5e62246c08f241ea10292b.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (e95729b84aad8afdbdf6538b03b16d3b) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3e1b18764dcdc88f0fa4a340ab9e9ec5), deploy into slot with allocation id 3b08d72bbe5e62246c08f241ea10292b.
Activate slot dd38a788566223d839535c438ad66311.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3e1b18764dcdc88f0fa4a340ab9e9ec5) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3e1b18764dcdc88f0fa4a340ab9e9ec5) [DEPLOYING].
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Received task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5ea13f14c09de7c28ba0c13cda57db87), deploy into slot with allocation id dd38a788566223d839535c438ad66311.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5ea13f14c09de7c28ba0c13cda57db87) switched from CREATED to DEPLOYING.
Loading JAR files for task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5ea13f14c09de7c28ba0c13cda57db87) [DEPLOYING].
Activate slot 51d153be151e91f4224db1d9e4f09599.
Activate slot 0921a68b65560381fae70f98eafc3786.
Activate slot 80c9f83e4f787e5cad6f4b52932e4599.
Activate slot 64da75a54b0f87824e6a8c5001252f87.
Activate slot 3b08d72bbe5e62246c08f241ea10292b.
Activate slot dd38a788566223d839535c438ad66311.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (e95729b84aad8afdbdf6538b03b16d3b) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (fd62ab53c3b574531f76e7764ee7b5b2) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (dd8bd296344e4011f0eee3a374a6f70c) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3e1b18764dcdc88f0fa4a340ab9e9ec5) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (0e52a8b7fde2ea1d7ae70facb94e4d27) switched from DEPLOYING to INITIALIZING.
Using job/cluster config to configure application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using application-defined state backend: MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880)
Using legacy state backend MemoryStateBackend (data in heap memory / checkpoints to JobManager) (checkpoints: 'null', savepoints: 'null, maxStateSize: 5242880) as Job checkpoint storage
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5ea13f14c09de7c28ba0c13cda57db87) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (fd62ab53c3b574531f76e7764ee7b5b2) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (e95729b84aad8afdbdf6538b03b16d3b) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (dd8bd296344e4011f0eee3a374a6f70c) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (3e1b18764dcdc88f0fa4a340ab9e9ec5) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (0e52a8b7fde2ea1d7ae70facb94e4d27) switched from DEPLOYING to INITIALIZING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (5ea13f14c09de7c28ba0c13cda57db87) switched from DEPLOYING to INITIALIZING.
Failed to trigger checkpoint for job 13a73f6a92cbf28074cfcfa7f707035e since Checkpoint triggering task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) of job 13a73f6a92cbf28074cfcfa7f707035e has not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (0e52a8b7fde2ea1d7ae70facb94e4d27) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6)#0 (e95729b84aad8afdbdf6538b03b16d3b) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3e1b18764dcdc88f0fa4a340ab9e9ec5) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6)#0 (dd8bd296344e4011f0eee3a374a6f70c) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5ea13f14c09de7c28ba0c13cda57db87) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6)#0 (fd62ab53c3b574531f76e7764ee7b5b2) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (3e1b18764dcdc88f0fa4a340ab9e9ec5) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (4/6) (e95729b84aad8afdbdf6538b03b16d3b) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (0e52a8b7fde2ea1d7ae70facb94e4d27) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (3/6) (dd8bd296344e4011f0eee3a374a6f70c) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (5ea13f14c09de7c28ba0c13cda57db87) switched from INITIALIZING to RUNNING.
Source: kafka source -> Map -> Sink: Print to Std. Out (2/6) (fd62ab53c3b574531f76e7764ee7b5b2) switched from INITIALIZING to RUNNING.
The following partitions have been added to the Kafka cluster. [flinkmsg2-1, flinkmsg2-2, flinkmsg2-0]
Assigning splits to readers {1=[[Partition: flinkmsg2-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]], 2=[[Partition: flinkmsg2-1, StartingOffset: -2, StoppingOffset: -9223372036854775808]], 3=[[Partition: flinkmsg2-2, StartingOffset: -2, StoppingOffset: -9223372036854775808]]}
Adding split(s) to reader: [[Partition: flinkmsg2-1, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Adding split(s) to reader: [[Partition: flinkmsg2-0, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Adding split(s) to reader: [[Partition: flinkmsg2-2, StartingOffset: -2, StoppingOffset: -9223372036854775808]]
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 2afef2cc-e80b-49cf-9be5-673e00778bef-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2afef2cc-e80b-49cf-9be5-673e00778bef
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 2afef2cc-e80b-49cf-9be5-673e00778bef-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2afef2cc-e80b-49cf-9be5-673e00778bef
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 1000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.5.153:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 2afef2cc-e80b-49cf-9be5-673e00778bef-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 2afef2cc-e80b-49cf-9be5-673e00778bef
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.2
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

Closing Source Reader.
Closing Source Reader.
Closing Source Reader.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (0e52a8b7fde2ea1d7ae70facb94e4d27) switched from RUNNING to FINISHED.
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5ea13f14c09de7c28ba0c13cda57db87) switched from RUNNING to FINISHED.
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 (0e52a8b7fde2ea1d7ae70facb94e4d27).
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3e1b18764dcdc88f0fa4a340ab9e9ec5) switched from RUNNING to FINISHED.
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 (5ea13f14c09de7c28ba0c13cda57db87).
Freeing task resources for Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 (3e1b18764dcdc88f0fa4a340ab9e9ec5).
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (1/6)#0 0e52a8b7fde2ea1d7ae70facb94e4d27.
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (6/6)#0 5ea13f14c09de7c28ba0c13cda57db87.
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
Un-registering task and sending final execution state FINISHED to JobManager for task Source: kafka source -> Map -> Sink: Print to Std. Out (5/6)#0 3e1b18764dcdc88f0fa4a340ab9e9ec5.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Source: kafka source -> Map -> Sink: Print to Std. Out (1/6) (0e52a8b7fde2ea1d7ae70facb94e4d27) switched from RUNNING to FINISHED.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
The configuration 'deserializer.encoding' was supplied but isn't a known config.
Kafka startTimeMs: 1636961458263
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961458263
The configuration 'deserializer.encoding' was supplied but isn't a known config.
The configuration 'client.id.prefix' was supplied but isn't a known config.
The configuration 'partition.discovery.interval.ms' was supplied but isn't a known config.
Kafka version: 2.6.1
Kafka commitId: 6b2021cd52659cef
Kafka startTimeMs: 1636961458265
Received resource requirements from job 13a73f6a92cbf28074cfcfa7f707035e: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=5}]
Source: kafka source -> Map -> Sink: Print to Std. Out (6/6) (5ea13f14c09de7c28ba0c13cda57db87) switched from RUNNING to FINISHED.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Reader received NoMoreSplits event.
Starting split fetcher 0
Starting split fetcher 0
Received resource requirements from job 13a73f6a92cbf28074cfcfa7f707035e: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=4}]
Starting split fetcher 0
Source: kafka source -> Map -> Sink: Print to Std. Out (5/6) (3e1b18764dcdc88f0fa4a340ab9e9ec5) switched from RUNNING to FINISHED.
Received resource requirements from job 13a73f6a92cbf28074cfcfa7f707035e: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-3, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Subscribed to partition(s): flinkmsg2-2
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-1, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Subscribed to partition(s): flinkmsg2-0
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-2, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Subscribed to partition(s): flinkmsg2-1
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-1, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Seeking to EARLIEST offset of partition flinkmsg2-0
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-2, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Seeking to EARLIEST offset of partition flinkmsg2-1
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-3, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Seeking to EARLIEST offset of partition flinkmsg2-2
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-1, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-2, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-3, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Cluster ID: JGb_UXIDSAWvTi9iBDO_1Q
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-2, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Resetting offset for partition flinkmsg2-1 to offset 0.
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-1, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Resetting offset for partition flinkmsg2-0 to offset 0.
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-3, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Resetting offset for partition flinkmsg2-2 to offset 0.
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-3, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Discovered group coordinator slave01:9092 (id: 2147483557 rack: null)
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-1, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Discovered group coordinator slave01:9092 (id: 2147483557 rack: null)
[Consumer clientId=2afef2cc-e80b-49cf-9be5-673e00778bef-2, groupId=2afef2cc-e80b-49cf-9be5-673e00778bef] Discovered group coordinator slave01:9092 (id: 2147483557 rack: null)
Failed to trigger checkpoint for job 13a73f6a92cbf28074cfcfa7f707035e since some tasks of job 13a73f6a92cbf28074cfcfa7f707035e has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 13a73f6a92cbf28074cfcfa7f707035e since some tasks of job 13a73f6a92cbf28074cfcfa7f707035e has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
Failed to trigger checkpoint for job 13a73f6a92cbf28074cfcfa7f707035e since some tasks of job 13a73f6a92cbf28074cfcfa7f707035e has been finished, abort the checkpoint Failure reason: Not all required tasks are currently running.
